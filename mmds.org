#+SETUPFILE: setup/theme-readtheorg.setup
#+EXPORT_FILE_NAME: dist/mmds.html
#+OPTIONS: toc:3
#+TITLE: Mining of massive data sets course notes

* Week 1 Map Reduce Paradigm
Whats so cool about this?
MapReduce paradigm emerged to make cluster computing easier (from a programming perspective).
The goal was to abstract out the various difficulties in cluster computing like:
Handling node failures, complexities of distributed computing.

** MapReduce Programming Model:
Map -> Group by Key -> Reduce 

Map Function: Takes the input and returns (key, val) pairs.
Group by Key: Combine same keys
Reduce: Do some transformation and return result for the (key, [v1 ,v2 ...]) group.

** Scheduling and Data Flow
   MR system partitions the input data and stores them on multiple nodes.
   Schedules the programs execution, and inter-node communications.
   Files are redundantly chunked and stored on multiple nodes.
   
   There is a masternode that has all the file metadata and takes care of the coordination.
   Master is typically not replicated, so when it fails, the task is aborted.
   
*The input and final output* are stored in the dfs.
*intermediate results* are stored on the map/reduce worker nodes.

*** Number of Map And Reduce tasks required per job
M map tasks, R reduce tasks.
Rule of thumb:
 M much larger than the number of nodes in the cluster.
 R is typically smaller than M.
The output is spread across the number of reducer nodes, fewer the better.

Multiple nodes do map and reduce tasks.
** Refinements
*** Combiners
Between Mappers and Reducer.
To save network time, the combiner pre-aggregates/transforms values with same key before sending to reducer.
ex) In the wordcount example, if from the map task running on a single node, we get (foo, 1), (foo, 1).
The combiner on that node, will combine them to (foo, 2). And only this value is sent to the reducer.

Programmer provides the combinde function :: (K, V1) -> V2
*Note* Combiner trick works only if the reduce fn is commutative and associative.
ex) sum fn can use the combiner trick,
    but average fn cannot. (because average fn is not commutative and associative) We can instead make the combiner send the total sum and number of values. The reducer can use this 
and compute the average.

*** Partition Function
This is provided by the map reduce system.
However, sometimes its useful to override the default partition fn. (Check the hostname/url example in the slides)

* Week 1 Analysis of large graphs
** Link Analysis and Page Rank
We'll Analyze the webgraph as an example
Main idea:
Compute important scores for nodes in a graph.
These methods are called link analysis:
- page rank
- hubs and authorities (hits)
- topic-specific (personalized) pagerank
- web spam detection algorithms
  
*** Mathematical Formulation for the Page Rank algorithm

**** Flow formulation
     Basic Idea:
Think of in-links as votes.
The in-link from a important page has higher weight than one from a less important page.
Recursive formulation!!

If page 'j' has importance r_j and and 'n' out-links, 
- weight of each out-link is r_j/n

The rank r_j for page j is
r_j = \sum r_i/d_i \forall nodes i that point to j with rank r_i and out-degree d_i

Add an addition constraint \sum r_j \forall j in graph to be 1. (This enforces uniqueness)

But this methods requires solving a bunch of equations. Guassian Elimination will work for small graphs, but not at the webscale!!

**** Matrix formulation
The above formulation can be rewritten as a eigen-vector formulation.

If page i has d out-links
Matrix M_ji = 1/d_i where d_i is the out-degree of node i.

The Matrix M is column stochastic as every column sums to one.
i.e) column i represents all the out-links from i. 
Thus the page rank formulation can be written as r = Mr.
Rank vector r is the eigen vector of the web-matrix M.

This can be efficiently computed using many methods. One such method is power-iteration.

Power Iteration:
- Start with r_0 = [1/N, 1/N ...]^T
As there are N webpages.
- r_1 = Mr_0
- Keep Iterating unitl (r_1 - r_0) < \epsilon
- r_1 - r_0 is L1 norm, it can be any vector norm.

**** Random Walk interpretation of Page Rank
Another way to think about page rank is using the idea of a random surfer.
If the random surfer is at a page 'i' at time t, what is the probability that he is in a page 'j'
at time t+1.
This is p_i(t)*(1/d_i)

Let p(t) be a vector that is the probability distribution at time 't' \forall nodes.
p(t+1) = Mp(t)

This is the same as the eigen value formulation above!!
When p(t+1) = Mp(t) = p(t), this is a stationary distribution.
Thus r is the stationary distribution for the random walk.

***** [[https://www.youtube.com/watch?v=uvYTGEZQTEs&list=PLANMHOrJaFxPMQCMYcYqwOCYlreFswAKP][Markov Processes]]
Central idea from the theory:
For graphs that satisfy certain conditions, the distribution will reach a stationary 
distribution irrespective of the initially probability distribution at t=0.

From the youtube videos linked above,
regular markov chain are the ones that have a stationary distribution.
regular markov chains are ones in which some power of the transition matrix has only
positive entries.
*absorbing states*
states of a transition matrix, in which it impossible to exit a state after entering it.


**** Google formulation of Page Rank
     we have our equation r_j^(t+1) =  Mr_j^t
Does this converge to what we want? is it reasonable?
It will converge if there are 
- no absorbing states (dead ends)
- no spider traps (small part of the graph which is not connected to anyother node,
 the surfer keeps jumping withing that part of the graph)

These 2 issues can be addressed by random-teleportation. 

Thus, in every iteration step:
- The surfer follows the links with p(\beta) and teleports to any random node with probability (1 - \beta).

Why does this make our power iteration converge?
 
From the markov chain theory, p_(t+1) = \pi * p_t
This will converge if \pi is
- Stochastic
- Aperiodic
- Irreducible (probability of going from one state in the graph to any other state is non-zero)
 
Adding random teleportation provides these conditions.

*** How do we really compute page rank in web-scale graphs??
    
    r_(t+1) = Ar_t
    
This would work great if we can store A in main memory.
Every entry in A will be non-negative, so this is going to takeup huuuuuuge amount of memory
boe:
- 1b webpages, 4 bytes per page (say)
  - r vectors will need 2 * 1b = 2billion entries.
  - matrix M will need 10^(9*2), wow!

Another way to formulate this:
- The matrix M is quite sparse.
- The random teleportation from a node j, adds (p(1-\beta)/N) in column j for every node in the graph, and reduces probability to (\beta)/N in column j of matrix M.
- i.e Tax each page (1-\beta) of its score, and distribute it evenly among all nodes.

r = A.r where A_(ij) = \beta M_(ij) + (1-\beta)/N

r_i = \sum_(j=1)^N A_(ij).r_j

A_(ij) = \sum_(j=1)^N (\beta M_(ij)*r_j + (1-\beta)/N * r_j)
A_(ij) = \beta \sum_(j=1)^N M_(ij)*r_j + (1-\beta)/N (as \sum_(j=)^N r_j = 1)

thus r = \beta M .r + (constant)
Thus, we can work with the sparse matrix M, and add a constant to every entry in each power iteration step.

Awesomatic Algorithm:
- while \sum_(j)(r_j^(t+1) - r_j^(t) > \epsilon) 
  - initialize r_j^0 = 1/N, t = 1
    - \forall j, r_j^(t) = \sum_(i->j)^N \beta * r_i^(t-1)/d_i
      - r_j^(t) = 0 if in-degre of j is 0
    - Re-insert the leaked page rank
    - \forall j: r_j^(t) = (1-S)/N where S = \sum_(j=1)^N r_j
    - t = t+1
      
* Week 2 Similar Sets LSH 
** Finding Similar Sets
   We'll focus on document similar in web-scale as an example.
   

   Its good to find similar sets, lot of cool applications like
recommendations, plagiarism, entity resolution.
3 essential techniques for similar documents:
1. Shingling
2. Minhashing
3. Locality Sensitive Hashing (LSH)

*** Shingling
    k-shingle for a document is a set of k characters that appears in that document.

Documents that are intuitively similar will have many shingles in common.
Shingles can be hashed to reduce storage bytes.

*** Minhashing
    This technique is used to uniquely represent each document by a vector.
The idea:
to start with, a *boolean matrix* is used to represent documents with various elements. (B)

Ex) rows = all possible K shingles.
    columns =  sets. (documents in our case)
- row e, col s has a 1 in it, only if element e belongs to set (document) s. 

Typically, this matrix is pretty sparse.

consider a set of minhash functions H.
Each fn:
- creates a random permutation of the rows, say r_p.
- h(c) = the first number n_p in r_p for which  B(n_p, c) = 1.
  
The Signature of the set (H(s)) is a vector with of numbers obtained from the various hash functions in H.
ex)
H = {h1, h2, h3, h4}
H(s) = [h1(c), h2(c), h3(c), h4(c)]

This is done for all columns to obtain the signature matrix S from the boolean matrix and H.

*Surprising property*
given 2 columns, c1 and c2, p(h(c1) = h(c2)) = Sim(c1, c2)

**** Similarity of signatures: /fraction of minhash fns for which h(c1) = h(c2)/
The expected value of Sos is the jaccard similarity of the 2 columns!!

**** Permuting the rows
Creating a permuted set of rows is expensive. For 1 billion rows, we'll need 1 billion entries
to store one permutation. 
Instead, here is a good approximation to permutation:
- Pick a set of normal hash fns. h1, h2, h3...
- Keep a slot M(i, c) for ith permutation and column c.
- M(i, c) will have the smallest value of h_i(r) for which row r has 1 in column c.
  
h_i corresponds to the ith permutation of the rows.

Algorithm:
- for each row r:
  - for each hash fn h:
    - compute h(r)

  - for each column c:

    - if B(r, c) is 1

      - for each hash fn h:

	- if h(r) < M(i, c)

	  - M(i, c) = h(r)








  



*** LSH
    Basic idea:
Rather than comparing all pairs of elements (docs), compare only the ones that have a
decent chance of being similar. (called *candidate pairs*)

in our case,
- pick a similarity threshold t, compare only those pairs that have a similarity greater than t.

How?
hash each doc to multiple buckets. Compare only those documents that are in the same bucket.

- Divide matrix M (Signature matrix) into b bands with r rows each.
- apply a hash fn to each band.
- Candidate pairs are those which fall in the same bucket for atleast one of the b bands.
- tune b and r to catch most similar pairs, but few nonsimiliar pairs.
  Intuitively,
larger b, smaller r is good if low similarity threshold
small b, large r is good if high similarity threshold

Look at the slides, to understand the math, but the relation is

   1 - (1 - s^r)^b
   t = (1/b)^(1/r)

This is a step function. We can tune b and r, based on the desired 't'. 

*** Applications of LSH
    
**** Entity Resolution
     misspellings, nicknames, change of address, phone numbers make entity resolution pretty interesting.
     Pretty cool application explained, in which jeff helped find similar records for 2 companies.
     
**** Fingerprint matching
     This again uses lsh, the nice trick in this one is the bucketing technique.
1024 sets of randomly selected 3 grid squares are taken from 2 fingerprints and compared.
If they match in any one of these sets, then they are compared. See the lectures as to why the 
numbers workout.

**** Duplicate news articles
     This uses a nice shingling technique. Stop words are used to get shingles. Idea is
similar articles will have common shingles, since ads dont have many stopwords. but
articles do.

* Week 2 K-Nearest neighbour learning
Supervised learning technique. y = f(x). predict y, for given features vector x.
Instance based learning: Keep whole training set. for a new query q, look at all x^' near q
and use that to predict y_q for q.
Collaborative filtering is a good example of k-NN finder.

Basic idea: /*Look at the K nearest nieghbours of our query point, and just predict the
avg output*/

Instead of looking at all points, to pick the nearest neighbors, just look at k-points 
near the query fn. how do you look at only a few points to figure out the k? LSH!!!
* Week 2 Frequent Itemsets
  Frequent Itemsets are items that appear together.
** Market-Basket Model:
*** Introduction and applications

Baskets, Items: stuff that are in a basket.
/*support*/ number of baskets that contain the same set of items.
/*support threshold*/ threshold number over which items are considered
 as /*frequent itemsets*/.

Applications examples:
baskets: sets of products, items: products
b: sentences, i: documents (plagiarism)
b: docs i: words (similar tweets/ other small docs)

Note: For good performance, there should be few items in a basket. (usually *quadratic in items* work done per basket)
*** Association rules

If a basket contains {i_1, i_2...i_k} -> j, it implies it likely contains j.
The degree of truth in this rule is the *confidence* of this rule. 
fraction of baskets that contain all of i_1-i_k and j vs those with i_1-i_k without j. i.e)  {i_1, i_2...i_k, j}/{i_1, i_2, ...i_k}

/*Support of an association rule is the support of the items on the left of arrow*/
*** finding association rules

question: /*Find all association rules, with support >= s and confidence >= c.*/

- find all frequent itemsets with support >= cs
- find all frequent itemsets with support >= s
- now, {i_1, i_2..i_k, i_(k+1)}, pick all subsets of this list with k items.
- if the support of {i_1, i_2 ... i_k} is s1 ~ >= s, then {i_1, i_2 ... i_k, i_(k+1)} is
a set with support s2 ~ cs. then the confidence of the association rule {i_1, ...i_k} -> j
is atleast c.
*** A-priori algorithm
2 pass algorithm for finding frequent itemsets.
main idea: If item i does not appear in s baskets, then no pair including i can appear
in s baskets.
algorithm:
2 pass algorithm
- in first pass, read all items and calculate count for each item. (i.e number of baskets in 
which the item appears)
- eliminate all items that have support < s.
- in second pass, read all pairs of items and its count.
- eliminate items that have support < s.
*** Improvements to A-priori
**** Part-Chen-Yu-Algorithm
     - In the first pass of a-priori, hash each pair to a bucket and keep (bucket, count) pairs
in memory.
     - In pass two, also check if the pair hashes to a bucket with count > support
**** Multistage algorithm
     - Add one more pass. create a middle pass that rehashes all the pairs (using diff hash fn)
in the first pass whose bucket counts were > s.
**** Multihash
     -  like pcy, but use multiple hashes in stage 1.

*** All (or most) frequent itemsets in <= 2 passes
    
**** SON algorithm
- pick random subset of all baskets that fit in memory.
- use a scaled back support s.
- do this multiple times for multiple random subsets.
- candidate pairs for second pass are those that satisfy the support in any one of the random subsets.
**** Toivonens algorithm 
- pick random subset of all baskets that fit in memory.
- use a scaled back support s. Further, the scaled back threshold is reduced slightly.
- Add to the itemsets obtained, the /*negative border*/ of these itemsets.
*negative border*
An itemset is in the negative border, if it is not deemed frequent in the sample,
but all its subsets are.

- run pass 2 which finds the frequent itemsets from all the data.
- If an itemset is in the negative border, and it turns out to be frequent, then we must 
start over again with another sample.

* Week 3 Community Detection
  Detect communities in a huge ass graph.
** Using Affiliation Graph Model and BIGCLAM
     Generative model for networkds
   c \in C, set of communities.
   v \in N, set of nodes.
   m \in M, memebership probabilities between n and c.
   AGM(V, C, M, {p_c})
   for each pair of nodes \in c, p_c is the probability that they are connected.
   P(u, v) = 1 - \pi_(c \in (M_u \cap M_v))(1 - p_c)
   for a pair of nodes u, v.
   This model helps us generate a network.
*** AGM to BIGCLAM
   Relax the agm model, each membership between node u and community a will have a strength F_ua.
   Each community A links nodes independently
P_A(u, v) = 1 - exp(-F_ua.F_va)
F = community membership strength matrix.
We have to find this matrix. (check slides for details)
** What makes a good cluster?
   Divide vertices into sets such that there are very few cross cluster connections
and lot of within cluster connections.
   - Use the idea of graph cuts to minimize this.
   - Will fail on some degenerate cases. (see slides)
   *Conductance*
\phi(A) = CUT(A)/Vol(A) 
Vol(A) = \sum_(i \in A)d_i. Lower the conductance, the better.
** Spectral Graph Partitioning
   Find clusters/communities in networks.
partition graph into pieces such that they have low conductance.
A: adjacency matrix of all nodes (n). (1 if connected, else 0)
x: vector of all nodes. (think of it as label/value of all nodes) 
what is A.x = y ??

y_i = A_i.x = \sum_(j = 1 to n) A_ij.x_j = \sum_(i, j \in E)x_ij
i.e y_i sum of all neighbours of i.

A.x = \lambda.x
Spetral Graph Theory:
Analyze the "spectrum" of matrix representing G.
spectrum: eigenvectors x_i of a graph, ordered by the magnitude of their
corresponding eigen values \lambda_i. \Lambda = {\lambda_1, \lambda_2 ... \lambda_n} \lambda_1 \le \lambda_2 ...

Graph Laplacian:
A: adjacency matrix.
D: degree matrix. it is a diagonal matrix of all degrees of nodes n.
Graph Laplacian Matrix L = D - A
n * n symmetrix matrix.
*** Spectral Graph partitioning
Fact:
For symmetric matrix M, second smallest eigen value is
\lambda_2 = min_x x^t(M)x / x^t.x

* Week 3 Streams
  Stream management is differenet from data management (from databases) in many ways.
  The input rate cannot be controlled, dont have all the time to do some computation.
** The Stream Model
   Two forms of quries:
   - /Standing queries/ 
queries asked all the time.
ex) output each new max value ever seen by the stream.

   - /Adhoc queries/
     One time queries.
ex) what is the max value seen so far in the stream.
*** Stream management system.
Consists of
Processor - (like esper) that allows for adhoc queries and standing queries.
Archival storage - not a dbms system, just stores the stream values so that the stream can be
reproduced in the future.
Limited working storage - (like flash/memory) that stores essential parts of the input stream
in a way that supports fast querying. 
*** Sliding window
    Useful model of stream processing in which queries are about stuff in a window.
    ex) averages. (avg of elements in the window)
    /Interesting case/: when the window cannot be stored in memory.
**** Counting 1's (when N does not fit in memory)
     Count the number of 1s for any k < N.
***** Algorithm that doesn't quite work:
       - Create blocks of 2, 4, 8, 16 ... upto log_2(N) looking backwards.
       - drop smaller blocks, if they begin at same point as a larger block,
or a larger block begins to its right.
       - Each block has the count of 1s in it.

****** Whats good in this algorithm?
       - O(log^2 N) bits.
	 - stores only O(log N) counts of log_2(N) bits each.
       - Easy updates as new bits come in. (we only have to modify O(log N) blocks)
       - Error in count no greater than number of 1's in the 'unknown area'.(some part of the last block)
******  Whats not good?
       - If all the ones are in the 'unknown area', error is unbounded.

***** DGIM method (rajeev motwani was one of them)
       Whats different in this?
 instead of creating exponential blocks based on the number of bits,
 do it based on the number of 1s seen in that block. i.e) block with 2 1s, block with 4 1s, 8 1s, 16 1s etc
*Bucket* it is a record that consists of:
- largest timestamp of the 1s in the bucket.
- number of ones between its beginning and end. (log log N) bits.
- constraint on buckets: number of 1s must be a power of 2, hence O(log log N) bits above. (i.e log N will be the number of ones, but instead of storing (log N) itself, we can store its log)
  
Look at slides for algorithm.
  
*** Bloom filters
   - filter streams only if the stream item is in someother list of items.
   - array of bits, set of independent hash fns.
   - when new input arrives in stream, pass it through the set of hash fns and set to 1 the
   array index returned by each of the hash fns. 
   - thus, the input has been seen if all these bits contain a one. (when querying)
*** Sampling streams
     - sample by value, not by position in stream.
ex) hash stream values to bunch of buckets. pick a few for the sample. (all/none of a stream value
is picked in the sample)
*** Counting distinct elements
    count all distinct elements in a stream.
    obvious algorithm: maintain set of elements in memory, and check for every new element
if it has been seen before.
**** flajolet-martin algorithm
     This algorithm is used to estimate the distinct elements in a stream.
     non-trivial idea:
use a hash fn that hashes every stream element to atleast (log_2 N) bits.
let r(a) = trailing 0s in h(a).
R = maximum r(a) over all a \in N.
estimate = 2^R. 
**** AMS method.
     Used for finding any moment, gives unbiased estimate.
     Based on multiple random variables.

* Week 5 Clustering 
** Hierarchical clustering
   Repeatedly combine two nearest clusters.
Start at each point as a cluster.
Notion of near? 
*** Euclidean case
Centroid: avg of all points
*** Non-Euclidean case
   Clustroid
   One of the points in the cluster, that is "closest" to all other points.
   What is closest?
- smallest max distance to all points
- smallest avg distance
- smallest sum of squares distance
Pick one based on application
*** When to stop clustering?
Threshold on diameter of cluster. (cohesion)
Density of cluster: no. of points/(some power of the radius)
When density lowers, stop merging.
       
       



Hierarchical clustering is pretty expensive. O(N^3) operations. using priority queues,
we can make it O(N^2 log N). not suitable for big datasets that do not fit in memory.
** K-Means algorithm
- Pick K points randomly as our cluster centers.
- For each new point, assign it to one of the k points that is nearest to it.
- re-calculate centeroid for this cluster.
- repeat until convergence.i.e points dont move much

K-means takes a long time to converge.
** Bradley-Fayed-Reina (BFR) algorithm
   Optimized for large datasets that dont fit in memory.
Scan dataset once, to cluster points.
Points are loaded in chunks into the memory, and summarized by some simple statitics.
*** Assumptions:
Points are normally distributed in euclidean space.
*** Algorithm
    First, pick some set of K points (chosen by sampling the huge dataset, and then applying
hierarchical clustering or something)
Three sets
**** Discard Set (DS)
     Points that are close to some centroid, can be used to summarize.
**** Compression Set (CS)
     Set of Points that are close together, but not close to any cluster.
**** Retained Set (RS)
* Week 7 LSH Improvements
** LSH families of Hash functions
   What is a hash fn in context of LSH?
   a fn that takes 2 arguments and returns if they are similar or not.
   Ex) LSH family = minhash, family of hash fns that compute minhash values and say whether 2 cols are similar.
   
A family H is said to be (d1, d2, p1, p2)-sensitive family if,
d1 is small and d2 is large.

for all d(x, y) < d1, p(h(x) = h(y)) > p1
for all d(x, y) > d2, p(h(x) = h(y)) < p2

S curve is such that, d2 - d1 is small, and p1 is large, p2 is small.
Checkout the minhash example in the slides. (1/3, 2/3, 2/3, 1/3)-sensitive family.
We can amplify the steepness of s curve by creating new lsh family by combining  multiple families. (AND, OR construction)
combining AND, OR construction, we get 1-(1-p^r)^b. The correct values of r and b will increase p1 and decrease p2.
*** Random Hyperplane family for cosine distance
    (d1, d2, 1-d1/180, 1-d2/180)-sensitive family

* Week 7 More about link analysis

