#+SETUPFILE: setup/theme-readtheorg.setup
#+EXPORT_FILE_NAME: dist/probability.html
#+OPTIONS: toc:3
#+TITLE: Probability Basics

* Probability models and axioms
2 Steps in any probabilistic model:
- Set of all possible outcomes
- Assign likelihood to each outcome
** Sample Space
- Set of all possible outcomes of an event.
- mutually exclusive, collectively exhaustive.
- The possible outcomes are of the right granularity.
The 'right' granularity depends on the experiment.
Ex) In an experiment that only cares if a coin is heads or tails, {head, tail} is of the right granularity compared
to {head and rains, head and not rain, tail  and rains, tail and not rain}
*** Types
Sample spaces can be continuous, discrete, finite, infinite.
*** Tree
A tree can be used to describe a sequential description of a sample space.
The leaves of the tree are each possible outcome in that sample space.
#+BEGIN_EXAMPLE

             /(H, T)
            /
           H---(H, H)
          /
         /
        .
         \
          \
           T---(T, T)
            \
             \
             (T, H)
#+END_EXAMPLE
** Probability Axioms
*** Event
A subset of the sample-space.
Probability is assigned to events in a sample-space.
Defining event as a subset of the sample-space allows us to generalize the idea for continuous and discrete sample spaces.
In case of continuous sample spaces, P(x) = 0 where x is a point. (Infinitesimal point calculus and limit fundaes!)
Whereas P(x) where a < x < b can be a number.
*** Rules/Axioms of probability
Probabilities by convention, are in the range between 0 and 1.
Intuitively, 0 means it practically cannot happen, 1 means that we
are practically certain that it would happen.

Axioms:
- Nonnegativity, $P(A) >= 0$
- $P(\Omega) = 1$ (i.e probability of the sample-space is 1)
- $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

If A and B are disjoint, $P(A \cup B) = P(A) + P(B)$

Only requirements for a probabilistic model to be valid!!
Interestingly, we don't need a requirement like P(A) <= 1 as this will always be the
case if the above axioms are satisfied.
*** Discrete Uniform Law
If a sample space consists of n equally likely events,
and A is a set that has k outcomes then

$$ P(A) = k.1/n $$

This is a very useful law, as it reduces a problem of computing probabilities to a
problem of counting.
*** Probability of a single point in a continuous sample space
Interestingly, P({x, y}) = 0.
i.e probability of a single point in the sample space of a continuous distribution is 0.
Another way to think of this is if probability corresponds to area,
the area of a single point is 0.
*** Probability Calculation Steps
    - Specify the sample space.
    - Specify a probability law
    - Identify events of interest (try to use pictures)
    - Calculate
*** Countable Additivity (strengthens the finite additivity axiom)
We know that for a set of finite disjoint sets
$$ P(A \cup B \cup C ...) = P(A) + P(B) + P(C) + ... $$

New Axiom:
If A1, A2, A3 ... is an infinite *sequence* of disjoint events,
then $$ P(A1 \cup A2 \cup A3 ...) = P(A1) + P(A2) + P(A3) + ... $$
**** Important to note
Infinite sets can be countable and uncountable.

For the uncountable set of points in a line, the above axiom does not hold.
ex)
Let us split a line to sum over all its points.
$P(\Omega) = P(\cup{x}) = \Sigma P({x}) = \Sigma 0 = 0$ as each point has a probability of 0.
but P(\Omega) = 1,

thus 1 = 0 ??

The key here is that $$ P(\cup{x}) != \Sigma P({x}) $$ as the elements of a line is *not countable*.
i.e its elements cannot be arranged in a *sequence*

"Area" is a legitimate probability law on the unit square, as long as we do not
try to  assign probabilities/areas to "very strange" sets. This involves deep mathematics
in an field known as "measure theory".
*** Uniform probability law for a countably infinite sequence
1) Let the sample space be the set of positive integers.
   Is it possible to have a â€œuniform" probability law,
   that is, a probability law that assigns the same probability c to each positive integer?

Ans) Nope.
Let c = 0.
1 = P(\Omega) = P(A1 \cup A2 \cup A3 \cup ...) = \Sigma P(A_i) = 0 + 0 + 0 = 0. Contradiction!

Let c > 0,
In this case, there is always a k such that k.c > 1.

Thus, we can never have a uniform probability law.

** Interpretting probabiilty theory
- Probability theory is just another branch of mathematics.
It has some axioms, we build models that satisfy these axioms, and we establish some
consequences that are the theorems of this theory.

*** But what is probability?
2 ways to think about it...
- Probability is roughly the frequency of an event
/Frequency of an event "A" is P(A)./
This way of thinking about probability works in some cases,
ex) Flipping a fair coin, P(Heads) = 1/2. i.e If we flip a coin infinitely many times,
whats the frequency of heads...
- Probability is a way of describing our beliefs
But beliefs are subjective!!

Well, probability gives us a systematic way of thinking about uncertain situations.

If our probability model is good, then it will help us make informed
decisions/predictions in the real world.

But how do we know that?
Statistics! It gives us data to verify our model and make it better.

#+BEGIN_EXAMPLE

  +-------------+         +-----------------------------+
  | Real World  |<--------|Probability Theory (Analysis)|
  +-------------+         +-----------------------------+
        \                      ^
         \                    /
          \                  /
           \ Data           / Models
            \              /
             \            /
              v          /
          +--------------------+
          |Inference/Statistics|
          +--------------------+

#+END_EXAMPLE

** Union Bound and Bonferroni inequality
*** Union Bound
Idea:
- few students are smart, A
- few students are beatiful, B
Then few students are smart or beautiful.

$$ P(A \cup B) <= P(A) + P(B) $$

*** Bonferroni inequality
Idea:
- Most students are smart
- Most students are beautiful
Then most students are smart and beautiful

$$ P(A \cap B) >= P(A) + P(B) - 1 $$
$(A \cap B)$ is close to 1, hence big.

Proof:
Need to show P(A \cap B) is pretty big.
Another way to show this is to say P((A \cap B)^c) is small.
$$ P((A \cap B)^c) = P(A^c \cup B^c) <= P(A^c) + P(B^c) $$ (by de morgans law and union bound)
$$ 1 - P(A \cap B) <= 1 - P(A) + 1 - P(B) $$
$$ - P(A \cap B) <= - P(A) - P(B) + 1 $$

$$ P(A \cap B) >= P(A) + P(B) -1 $$

General case:
$$ P(A \cap B \cap C ...) <= P(A) + P(B) + P(C) + ... (n-1) $$ where n is the number of events.
i.e if all the events are big, then the intersection of them is close to 1.
* Conditioning and Independence
** Conditioning and Bayes' rule
Idea of Conditioning: Use information to revise our model.

$$ P(A/B) = P(A \cap B) / P(B)  $$

This is a fairly intuitive definition. If event B already occured, the
chance of event A also having occured, is basically the proportion of
outcomes in A \cap B to the proportion of events in B.
i.e what percent of events in B also occurs in A.

Conditioning doesn't change the relative frequencies of outcomes. If
all outcomes are equally likely in the original universe, they
continue to be equally likely in the conditional world.

(This simplifies calculations, for discrete outcomes). In the case
that all probabilities in our conditional universe are equally likely,
P(A/B) reduces to a case of counting events. (A \cap B) / B. (By the
discrete uniform law)
*** Conditional Probabilities satisfy the axioms of probability.
- Namely, P(A/B) >= 0 assuming P(B) > 0.
- P(\Omega/B) = 1
- If A \cap C = \empty then  P(A \cup C/B) = P(A/B) + P(C/B)

This has important implications. Any properties that we derive for
probabilities, also applies to conditional probabilities.
*** Multiplication Rule
P(A \cap B) = P(A/B) * P(B)
         = P(B/A) * P(A)

This just comes from the conditional probability defined above.

Extending this to three events, as shown in the tree diagram:

#+BEGIN_EXAMPLE




                 (A \cap B \cap C)
                /
               / P(C / A \cap B)
              /
             /
            B----------------------------- (A \cap B \cap C^c)
           /          P(C^c / (A \cap B)
          /
         / P(B/A)      ---------------------------- (A \cap B^c \cap C)
        /            /        P(C / (A \cap B^c)
       A--------- B^c
      /     P(B^c/ B) \
     / P(A)            ---------------------------- (A \cap B^c \cap C^c)
    /                           P(C / A \cap B^c)
   /
  .
   \
    \
     \ P(A^c)                     P(C/ A^c \cap B^c)
      \                      --------------------- (A^c \cap B^c \cap C)
       \                    /
        A^c -----------------B^c
         \    P(B^c/A^c)    \
          \                  --------------------- (A^c \cap B^c \cap C^c)
           \                      P(C^c / A^c \cap B^c)
            \ P(B/A^c)
             \
              B------------------ (A^c \cap B \cap C^c)
               \  P(C^c/A^c \cap B)
                \
                 \
                  \
                   \ P(C/ A^c \cap B)
                    \
                     \
                      (A^c \cap B \cap C)

#+END_EXAMPLE

P(A \cap B \cap C) = P(A) * P(B/A) * P(C / A \cap B)

Extending this,

General multiplication rule

*P(A_1 \cap A_2 \cap A_3 ... A_n) = P(A_1) * \prod_(i=2 to n) P(A_i / A_1 \cap A_2 ... A_(i-1))*
*** Total probability theorem
This is an interesting approach for finding the probability of an event. Divide and conquer approach.

To find the probability of an event 'B', we partition our sample space into multiple
simple partions A1, A2, A3 (say 3) and find P(A_i \cap B) for i=1,2,3 and sum them up.

This can be generalized to

$$ P(B) = \sum_{i=1}^{\infty} P(B/A_i)P(A_i) $$
i.e, probability of event B is the weighted average of P(B/A_i) with P(A_i) as the weights.

*** Baye's Rule
Systematic approach for incorporating new evidence.
i.e, how do we re-evaluate our beliefs based on some event.

**** Bayesian Inference
- Initial beliefs P(A_i) on possible causes of an observed event B.
- model of world under each A_i: P(B/A_i)

#+BEGIN_EXAMPLE

   +-----+    model         +----+
   | A_i | -------------->  | B  |
   +-----+    P(B/A_i)      +----+

#+END_EXAMPLE


- draw conclusions about the causes

#+BEGIN_EXAMPLE

   +-----+    model         +-----+
   | B   | -------------->  | A_i |
   +-----+    P(A_i/B)      +-----+

#+END_EXAMPLE


      P(A_i/B) = P(B \cap A_i)/P(B) = P(A_i).P(B/A_i)/P(B)

We can get P(B) using the total probability theorem above.
** Independence
If P(B/A) = P(B), this means that knowledge of A does not change
our belief about B.

P(A \cap B) = P(A) * P(B/A) = P(A) * P(B).

If A & B are indepent, then P(A \cap B) = P(A).P(B)

Independence is very different from disjoined. Disjoint events are always dependent, as
knowing one, means the other cannot occur.
Independence is a relation about information.

Independence is a powerful idea that helps us breakdown an event into a set of independent
events.
*** Independence of event complements

If A and B are independent, then A and B^c are independent.
Intuitive, since if occurence of A doesn't tell us anything about B,
it doesn't tell us anything about B^c as well.

Proof:
\begin{align*}
P(A \cap B) &= P(A).P(B) \\
P(A) &= P(A).P(B) + P(A \cap B^c) \\
P(A \cap B^c) &= P(A) - P(A).P(B) \\
&= P(A)(1- P(B)) \\
&= P(A).P(B^c) 
\end{align*}

Similarly, (and by symmetry)

A and B are independent => A and B^c  => A^c and B => A^c and B^c are independent.
*** Conditional independence

P(A \cap B/C) = P(A/C).P(B/C)

Does independence imply conditional independence??

No! Independence does not imply conditional independence, this can be
shown using a venn diagram.

However, given something is conditionally independent, the properties of independence
still hold. (like the event complements explained above)
*** Independence of a collection of events
Intuitive definition: Information on some set of events, does not affect of beliefs on
other set of events in the colleciton.

i.e Events A1, A2, ... An are called independent if:

P(A_i \cap A_j ... \cap A_m) = P(A_i).P(A_j)...P(A_m) for any distinct i, j ... m.

There is a tedious proof that formally shows that the above definition implies independence.

*** Self indepence
Is P(A) independent of P(A/A)?

Yes, only if P(A) = 0 or 1.

Is P(A \cap B/B^c) given A \cap B are independent?
nope, as B is present in both the numerator and denominator, knowledge of B implies
B^c didnt occur.
*** Independence vs Pairwise Independence
Consider the following example,
we have 2 independent coin tosses,

H1 = first toss  is heads
H2 = second toss is heads

C = Both tosses are the same.

P(H_1) = 1/2 = P(H_2)

P(C) = P(HH) + P(TT) = 1/2
**** Is C independent of H1?
P(C \cap H1) = P(H1 \cap H2) = 1/4
P(C \cap H1) = P(C).P(H1) = 1/2 * 1/2 = 1/4.
Yes, they are independent.
**** Is C independent of H2?
Yup, by the same argument above.
**** Are C, H1, H2 independent?
hmmm,
C-H1, C-H2, H1-H2 are independent.
i.e they are pairwise independent.

But is P(C/H1 \cap H2) = P(C)? nope, P(C/H1 \cap H2) = 1.
P(C \cap H1 \cap H2) = P(HH) = 1/4
P(C).P(H1).P(H2) = 1/8.
Thus, they are not independent!!
** A coin tossing puzzle
Alice tosses a coin, and claims that the event of getting 2 heads is
atlease as likely if we know that the first toss is heads than if we
know that atlease one of the tosses is heads.

Let's define the following events:

A: First toss is heads
B: Second toss is heads
C: Atlease 1 toss is heads

Alice's claim:

$$ P(A \cap B | A) \ge P(A \cap B | A \cup B) $$

$$ P(A \cap B | A) = P(A \cap B)/P(A) $$

\begin{align*}
P(A \cap B | A \cup B) &= P(A \cap B \cap (A \cup B))/P(A \cup B) \\
&= P(A \cap B)/P(A \cup B)
\end{align*}

Since P(A) \le P(A \cup B), the inequality stated by alice holds.

More generally, given Events C, D, E s.t:
1) D \sub E
2) C \cap D = C \cap E

P(C|D) \ge P(C|E) (not very interesting, but this is just the general case alice's claim)
** Monty Hall Problem
The Monty Hall problem. This is a much discussed puzzle, based on an
old American game show. You are told that a prize is equally likely to
be found behind any one of three closed doors in front of you. You
point to one of the doors. A friend opens for you one of the remaining
two doors, after making sure that the prize is not behind it. At this
point, you can stick to your initial choice, or switch to the other
unopened door. You win the prize if it lies behind your final choice
of a door. Consider the following strategies:

Stick to your initial choice.

Switch to the other unopened door.

You first point to door 1. If door 2 is opened, you do not switch. If
door 3 is opened, you switch.

Which is the best strategy?

There is a lot of literature on this problem. ([[https://www.wikiwand.com/en/Monty_Hall_problem][wiki]]) But here is the
intuiteive idea. If he decides to stick to initial choice, chance of
winning = 1/3. We bank on the probability of getting initial guess
right. If he decides to switch, we ban on the probability of getting
the initial guess wrong, which is more likely. So its better.
* Counting
Counting is a very useful thing to know especially for finding probabilities.
As a large number of probability problems reduce to counting elements in a set.
** Basic Counting Principle
r stages, n_i choices in stage i.

then total number of possibilities n1.n2.n3...nr

i.e if there are r stages, and n_i choices for each stage, then total number of
distict possibilities is product of choices in each stage.

This is the basic counting principle.
** Permutation
Number of ways of arranging 'n' elements.

we have 'n' slots, and we want to put elements into these slots.
By the counting principle, the number of ways we can do this is

n.(n-1).(n-2)...1 = n!
** Number of subsets
If we have 'n' elements, how many distinct subsets can we form?

Well, each element has 2 choices, its either in the set or not in the set.

2^n subsets can be formed.
** Combination
we have 'n' elements, select a subset of 'k' elements of the original 'n' elements.

Notation: n choose k, (^n_k)
*** How?
Lets break this down using the counting principle.

We have 'k' slots, and we need to pick a element for each slot.

To construct an ordered list, we pick n.(n-1).(n-2)...(n-k+1) = n!/(n-k)!
However, in this we only need to choose the elements, no need to order them!
So lets divide this by the number of ways they can be arranged. In this case, thats k!. (k elements can we arranged in k! ways from the counting principle)

nC_k = n!/(n-k)!k!

** Binomial Coefficients
Binomial Coefficients (^n_k) -> Binomial probabilities
*** n >= 1 indepenedent coin tosses; P(H) = p P(k heads) = ??

P(k heads) = no. of k head sequences * P(single k head sequence)

This argument works only if P(single k head sequence) is for every sequence.

In this case P(k heads) = (^n_k) . p^k(1-p)^k.
** Partitions
Suppose we an 'n' items, and we need to split them among r people with n_i items given to person r_i.

How many ways can we do that? or in other words, how many ways can we partition these n items into r sets?
*** Hint: Think about how many ways we can order the n items.
n items can be ordered in n! ways.
Alternatively:
Deal n_i items to person i and then order.

Suppose there are C ways of distributing the 'n' items among r people,

then

C.(n1!.n2!.n3!...n_r!) = n!

Thus,

C (number of partitions) = n!/(n1!.n2!.n3!...nr!)

This is called the *multinomial coefficient*. For the special case of r=2, we get the binomial coefficient. n!/(n-r)!.r!
** Each Person gets one ace problem
We deal a deck of 52 cards among 4 people.
Find the probability that each person gets one ace?

Well, 2 ways to solve it.
*** first way, using a probability model.
outcomes are: partitions
all partitions are equally likely. hence we can use the uniform probability law and just count.

Partition the 52 cards among 4 people.

total ways = 52!/(13!.13!.13!.13!)

favorable ways = (4!).48!/(12!.12!.12!.12!)

P(each gets one ace) = favorable/total.
*** second way
lets arrange the 52 cards with the 4 aces on top.
now, distribute the aces such that each person gets 1.

1st Ace: there are 52 slots, it can go anywhere.
2nd Ace: there are 51 slots, it can go anywhere except the first person. 39/51
3rd Ace: 26/50
4th Ace: 13/49

P(each gets one) = (39/51).(26/50).(13/49)
** Birthday paradox
 there are n people, each person has a random birthday in one of the 365 days.
Find the P(no 2 birthdays coincide).

\Omega = All possible birthday combinations.
  = (365)^n

All combinations where no 2 have the same b'day

365.364.363.362...(365 - n + 1)

P(no 2 b'days coincide) = 365.364.363.362....(365 - n + 1)/365^n


Interestingly at around n = 23, this probability is about 50%.
** Rooks on a chessboard
8 rooks are placed on 8 positions on a 8 * 8 chessboard with all positions equally likely.
Find the probability that all are safe.

\Omega: All possible combination of placements.
 = 64C_8.

One example,
#+BEGIN_EXAMPLE

 * * * * * * * C
 * * * * * * C *
 * * * * * C * *
 * * * * C * * *
 * * * C * * * *
 * * C * * * * *
 * C * * * * * *
 C * * * * * * *

#+END_EXAMPLE

A: safe possible combinations = each rook in unique row and column
  = since there are 8 rows, we need to select a unique column across all rows.
  = 8.7.6.5.4.3.2 = 8!

P(A) = 8!/64C_8

Another way to think about the numerator, pick one safe possibility, and permute the
rows of this possibility. this can be done in 8! ways.
** Multinomial probabilities
An urn contains balls of r different colors.
we draw n balls, with different draws being independent. For a given draw, there is a
probability p_i, i = 1..r of obtaining a ball of color i.

Let n1, n2...n_r be nonnegative integers that some to 1.
What is probability that we obtain exactly n_i balls of type i for each i=1..r.

From the multinomial coefficient discussion, The number of ways of partitioning a set of n items,
 into r sets of size n1, n2...n_r is

C = n!/(n1!.n2!.n3!...n_r!)

For each of these sets, we want a sequence n_i balls of color i.
Thus probability of getting a particular combination of balls is p_1^(n1).p_2^(n2).p_3^(n3)...p_r^(nr)

Thus, p(get type (n_1, n_2 ... n_r)) = C.p_1^(n1).p_2^(n2).p_3^(n3)...p_r^(nr)
* Discrete random variables
** Probability Mass functions and Expectations
*** Random Variable
Consider a sample space of students 'a', 'b', 'c', 'd'.
W is a random variable that takes an outcome (student) and returns a value (weight in this case)

#+BEGIN_EXAMPLE

                   +------+
student ---------> |  W   | --------> weight 'kgs'
                   +------+

#+END_EXAMPLE

A random variable (r.v) associates a 'value' (a number) to every possible outcome in \Omega.
- Mathematically: Its a function from the sample space \Omega to the real numbers.
- It can take discrete or continuous values.
- A function of one or several r.vs is also a r.v.
ex)meaning of X + Y: is a r.v that takes the value x + y, when X takes the value x and Y takes the value y
*** Probability Mass function (PMF) of a discrete r.v X
- Its the probability that the random variable X takes on a value x, P_X(x) = P(X = x)
- The PMF P_X is a function of an argument 'x', for any x, it specifies the probability that
  the random variable X takes on this particular value of x.

Consider a sample space of {a,b,c,d} and a random variable X which can take values (1,2,3).
The \Omega has p(event) = 1/4 for all values.

If X(a) = 1, X(b) = 1, X(c) = 2, X(d) = 3.

Then P_X(1) = {w \forall w \in \Omega where X(w) = 1} = P({a \cup b}) = 1/2.
**** Properties
- P_x(x) >= 0
- \Sigma_x P_X(x) = 1
*** Random variable vs number
Let X be a random variable that takes numeric values, with PMF P_X(x). Let Y be
another integer-valued r.v and let y be a number.

- P_x(y) is a number.
- p_X(Y) is a random variable.
why?
Y is a r.v that maps w in the sample space to an integer y.
for different y \in Y(w), p_X(Y) maps y to p_X(y)
i.e p_X(Y) maps \Omega to {p_X(Y(w)) | w \in \Omega}
*** Bernoulli and indicator random variables
- Random variables that model "success/failure", "heads/tails" etc...
- X {1 w.p p, 0 w.p 1 - p}.
Bernoulli with parameter p \in [0, 1]
- Indicator r.v of an event A: I_A = 1 iff A occurs
*** Discrete uniform R.V
    *parameters*: a, b. [a, b] \\
    *experiment*: pick a, a+1, a+2...b all equally likely with probability 1/(b-a+1) \\
    *Sample Space*: a, a+1 .. b. Number of values = (b-a+1) \\
    *model of*: complete ignorance! \\
    *R.V X*: X(w) = w \\
    *PMF*: p_X(i) = 1/(b-a+1) where a <= i <= b \\
*** Binomial R.V
*parameters*: positive integer n; p \in [0, 1] \\
*Experiment*: n independent tosses of a coin with P(Heads) = p. \\
*Sample Space*: Set of sequences of 1 and 0, of length n. \\
*R.V X*: number of 1's that are observed. \\
*Model of*: Number of successes in a given number of independent trials. \\
*PMF*: $p_X(k) = {n \choose k}p^k(1-p)^(n-k)$
*** Geometric R.V
*Parameter*: 0 < p <= 1 \\
*Experiment*: Infinitely many independent occurrences of an event with probability p. \\
*note*: When we say infinitely many independent occurrences, we mean that any finite subset of the outcomes are independent. \\
*Sample Space*: Set of Infinite sequences of 1 and 0. \\
*Random Variable* X: Number of trials until first success. \\
*Model of*: Waiting times, number of trials until a success. \\
*PMF*: $P_X(k) = P(we get first success in kth trial) = (1-p)^(1-k).p$
** Expectation/Mean of a R.V
It is the average that you expect to see in a large number of random and independent
repetitions of an experiment.

$$ E[X] = \Sigma_x x.p_x(x) $$

*Note* If we have an infinite sum, it needs to be well-defined.
i.e We assume $\Sigma_x |x|.p(x) < \infin$
*** Expected value of Bernoulli R.V
Let I_A be a indicator r.v of the occurence of event A.
P(A) = p.

X = {1, w.p p; 0, w.p (1-p)}
E[X] = 1.p + 0.(1-p) = p

E[ I_A ] = p
*** Expected value of Uniform R.V
let X be a uniform r.v.
Uniform on 0...n.
p(x) = 1/(n-0+1) = 1/n+1

E[X] = 0/(n+1) + 1/(n+1) + 2/(n+1) + ... n/(n+1) = 1/(n+1) (0 + 1 + 2 + 3 ... n)
= n.(n+1)/2(n+1) = n/2

E.V is at the center of gravity of the distribution.
*** Expectation as population avg.
X is an R.V thats the weight of the student.
In a class of n students, where x_i is weight of ith student, and p(i) = 1/n
E[X] = (1/n).\Sigma_x x_i. = sum of weights/total = average.

Thus Expectation can be thought of either as the expected value over a bunch of repeated trials, or (as in this case)
average of a population.
*** The Expected Value Rule (for calculating E[g(X)]
Let X be a r.v, let Y be a r.v = g(X) where g is a function from X -> Y.
What is E[Y]?

E[Y] = E[g(x)] = \Sigma_x g(x).p_X(x)

- Proof

E[Y] = \Sigma_y(\Sigma_x where g(x) = y).p(x)
     = \Sigma_y y.\Sigma p(x)
     = \Sigma_y y.p(y)

*** Linearity of expectation
    E[aX + b] = aE[X] + b

    let g(x) = a.x + b.

    Let Y = g(x)

    E[Y] = E[g(x)] = \Sigma_x g(x).p(x) = \Sigma_x(ax + b)p_x(x) = a.E[X] + b
    E[g(x)] = g(E[X]) is true for some g's. (true when g is linear, for non-linear fns its generally false)
** Variance and conditioning on an event; multiple r.vs
*** Variance
Measure of spread of an r.v.

E[X - \mu] = E[X] - E[\mu] = \mu - \mu = 0.
i.e The average distance from the mean is 0. which makes sense.

variance = var(X) = E[(X - \mu)^2 ].
Avg of squared distance from the mean. >= 0.

\sigma_x = sqrt(var(X))

**** usefule formula for calculating variance
var(X) = E[(X - \mu)^2] = E[X^2 + \mu^2 -2.X.\mu] = E[ X^2 ] + E[X]^2 - 2.E[X]^2 = E[ X^2 ] - E[X]^2
**** Properties
     var(aX + b) = a^2.var(X)
***** Proofs

\mu = E[X]

- Let Y = X + b, \nu = E[Y] = E[X] + b
var(Y) = E[(Y - \nu)^2 ]
 = E[(X + b - (\mu + b))^2] = E[(X - \mu)^2 ] = var(X).

Thus, adding a constant does not change the .

- Let Y = a.X \nu = E[Y] = E[a.X] = a.E[X]
var(Y) = E[(Y - \nu)^2 ] = E[(a.X - a.E[X])^2 ] = E[a^2.(X - E[X])] = a^2.var(X)

*** Variance of Bernoulli R.V
var(X) = E[X^2 ] - E[X]^2 = p - p^2 = p(1-p)
This is a parabola, whose value is maximum when p = 0.5.
*** Variance of Uniform R.V
var(X) = E[ X^2 ] - E[X]^2
       = 1/(n+1) * (0^2 + 1^2 + ... n^2) - (n/2)^2
       = 1/(n+1) * (n/6 * n.(n+1)(2n+1) - (n/2)^2
       = n(n+2)/12

For a general u.r.v from a to b, b - a = n

Adding a constant does not change pmf,
thus var(X) = 1/12 * (b - a)(b - a + 2)
*** Conditional pmfs and expectations given an event

P_X|A (x) = P(X=x|A)

\Sigma_x p_X|A(x) = 1

conditional expectation: E[X|A] = \Sigma_x x.p_X|A(x)

E[g(X)|A] = \Sigma_x g(x).p_X|A(x)

Its pretty much the same, except that the pmf becomes the conditional pmf.

*** Total expectation theorem
Conditional probability allows us to divide our sample space into smaller and simpler
pieces and then find the event in question. (Divide and conquer!)

P(B) = P(B|A1).P(A1) + P(B|A2).P(A2) + P(B|A3).P(A3) ... + P(B|A_N).P(A_N)

Now, let event B = {X = x},

p_X(x) = P(A_1).p_X|A1(x) + P(A_2).p_X|A2(x) + ... + P(A_n).p_X|AN(x)

This makes sense, the pmf of X=x is the same as sum of pmf of X=x given A_i for i = 1 to n.

mulitplying by x on both sides and sum for all possible choices of x,

\Sigma_x xp_X(x) = P(A_1)\Sigma_x x.P_X|A1(x) + ... + P(A_n).\Sigma_X x.p_X|A_n(x)

E[X] = P(A_1).E[X|A1] + P(A_2)E[X|A2] + ... P(A_n).E[ X|A_n ]

Thus, we can divide and conquer expectations!

*** Geometric R.V is Memoryless
X: number of tosses until first head; P(H) = p.

p_X(k) = (1-p)^(k-1).p, k =  1,2 ...

Now say we joined the experiment after the first toss.
Our geometric distribution would be the same as that for X, with parameter p.

*Memorylessness*
Number of remaining coin tosses, conditioned on tails in the first toss, is Geometric
with parameter p.

More formally,
Conditioned on X > 1, X - 1 is a geometric r.v with parameter p.

P(X-1 = 3 | X > 1) = P(T2T3H4|T1)

As the tosses are independent, P(T2T3H4|T1) = P(T2T3H4) = P(X = 3) = (1-p)^2.p

Generalizing this argument,

P(X - 1 = k | X > 1) = P(X = k) = P(X - n = k | X > n)
*** Mean of Geometric R.V
Consider X as a geometric r.v with parameter p.

E[X] = \Sigma_k k.(1-p)^(k-1).p

This is a infinite sum, can we simplify this or think of it in someother way?

E[X] = 1 + E[X-1]

First toss + remaining tosses (true by linearity of expectation)

Sucess in the first trial or success in a later toss.

E[X] = 1 + p.E[X-1|X=1] + (1-p)E[X-1|X>1]
 = 1 + 0 + (1-p)E[X] (from the memorylessness property)

thus E[X] = 1/p

Fairly difficult calculations become easy if we break down the problem in a clever way.
*** Joint pmfs and expected value rule
X: p_X
Y: p_y

P(X = Y) ?

Joint PMF: p_(X, Y)(x, y) = P(X = x and Y = y)

\Sigma_x \Sigma_Y p_(X, Y)(x, y) = 1

p_X(x) = \Sigma_y P_(X, Y)(x, y)
p_Y(y) = \Sigma_x P_(X, Y)(x, y)

p_x and p_y are called marginal pmfs.
*** Functions of multiple r.vs
Z = g(X, Y)

p_Z(z) = P(Z=z) = P(g(X, Y) = z) = \Sigma_(x, y where g(x, y) = z) P_(X, Y) (x, y)

Expected value rule: E[g(X, Y)] = \Sigma_x \Sigma_y g(x, y).p_(X, Y)(x, y)
*** Linearity of Expectations for multiple r.vs

We know, E[aX + b] = a.E[x] + b

E[X + Y] = E[X] + E[Y]

E[X + Y] = E[g(X, Y)]
 = \Sigma_x \Sigma_Y g(x, y).p_(X, Y)(x, y)
 = \Sigma_x \Sigma_Y (x + y).p_(X, Y)(x, y)
 = \Sigma_x x.\Sigma_y p_(X, Y)(x, y) + \Sigma_y y.\Sigma_x p_(X, Y)(x, y)
 = E[X] + E[Y]

similarly,

E[A1 + A2 + ...] = E[A1] + E[A2] + ...

Example,

E[2X + 3Y - Z] = 2E[X] + 3E[Y] - E[Z]
*** The mean of the binomial
Let X be a binomial r.v which indicated number of successes in n independent trials
with parameters p, p.

E[X] = \Sigma_k k.(^n_k)p^k.(1-p)^(n-k)

This is a complex sum, can we simplify this?

Let X_i be a indicator r.v that indicates success in i^th trial.

X = X_1 + X_2 + ... X_n.

Thus E[X] = E[X1] + E[X2] + ... E[Xn] = np
** Conditioning on a Random Variable, Indenpendent r.v's
*** Conditioning on a R.V
    p_X|Y(x|y) = P(X=x|Y=y) = P(X=x, Y=y)/(P(Y=y) = p_(X, Y)(x, y)/p_Y(y) defined for y such that p_Y(y) > 0.

The way to think about this, is to fix y and think of the conditional pmf. \Sigma_x p_(X|Y)(x|y) = 1
*** Mulitplication rule for pmfs
p_(X, Y)(x, y) = p_Y(y).p_x(x/y) = p_X(x).P_Y(y/x)
*** Conditional expectation on r.vs
E[X|Y] = \Sigma_x x.p_X|Y(x|y)

Expected value rule,

E[g(X)|Y=y] = \Sigma_x g(x)p_(X|Y)(x|y)

Total probability and expectation theorem

Total probability theorem,

p_X(x) = P(A1).p_X|A1(x) + ... + P(An).p_(X|An)(x)

suppose we are given Y = {y1, ... yn} A_i = {Y=y_i},

P_X(x) = \Sigma_y p_Y(y).p_X|Y(x|y)

Similarly,

E[X] = P(A1).E[X|A1] + ... P(An).E[X|An]

E[X] = \Sigma_y p_Y(y).E[X|Y=y]
*** Independence of Random Variables
Independence of 2 events is
P(A \cap B) = P(A).P(B), P(A|B) = P(A)


For an r.v and a event A,

P(X = x and A) = P(X = x).P(A) for all x
P(A|X=x) = P(A), for all x.

For 2 random variables,

p_(X, Y)(x, y) = p_X(x).P_Y(y), p_X|Y(x|y) = p_X(x), p_Y|X(y|x) = p_Y(y)

What does this mean?

If we know something about y, our beliefs about x does not change.
*** Independence and expectations
In general,

E[g(X, Y)] \ne g(E[X], E[Y])

Exceptions: E[aX + b] = aE[x] + b

E[X + Y + Z] = E[X] + E[Y] + E[Z]

However, if X and Y are independent,

E[XY] = E[X].E[Y]


E[X/Y] = \Sigma_x \Sigma_Y x/y . p_X(x).p_Y(y)
       = E[X]E[1/Y]
*** Variance and independence
In general,
var(X + Y) \ne var(X) + var(Y)

If X, Y are independent

var(X + Y) = var(X) + var(Y)

Proof:
for the sake of proof, lets assume E[X] = E[Y] = 0

var(X + Y) = E[(X+Y - \mu)^2 ] = E[(X + Y)^2 ] = E[X^2 + Y^2 + 2XY]
 = E[ X^2 ] + E[ Y^2 ] + 0
 = var(X) + var(Y)


E[XY] = \Sigma_X \Sigma_Y xy.p_(X, Y)(x, y)
      = 0

var(X + Y) = E[ (X+Y)^2 ] = E[X^2 ] + E[Y^2 ] + 2.E[XY]
*** Hat Problem
n people throw their hats into a box and then pick one at random.

- All permutations are equally likely.
- Equivalent to picking one hat at a time

X: number of people who get their own hat back.

E[X] = \Sigma_x x.p_X(x)

p_X(x) is hard to figure out.

Instead, X_i = 1 if person i selects own hat, else 0.

X = \Sigma_x X_i

E[ X_i ]?

By looking at the first description of the problem, 'all permutations are equally likely', this description is symmetric w.r.t all the
persons, hence E[X_i ] = E[ X_1 ] = E[ X_2 ] ...

E[ X_1 ] = x.p(X_1 = 1) = 1/n

thus, E[X] = 1. easy!

Now, lets compute the variance of X.

X = X1 + X2 + ... Xn

however, X1, X2 etc are not independent, (knowing X1 tells us something about X2. Ex) Consider the 2 person case.)

thus,

var(X) = E[ X^2 ] - E[X]^2

E[ X^2 ] = E[ \Sigma_(i=j)X_i^2 + \Sigma_(i \ne j)X_i.X_j ]

 E[ X_i^2 ] = E[ X_1^2 ] = E[ X_1 ] = 1/n (As its a bernoulli r.v)

for i \ne j, E[ X_i.X_j ] = E[X1.X2] = E[X1=1 and X2 = 1] = 1/n . 1/(n-1)

Thus, var(X) = (1 + n(n - 1)/n(n-1)  (As there are n(n-1) cross terms where i \ne j)) - E[X]^2

var(X) = 2 - 1 = 1.
*** Inclusion Exclusion formula
This is a beatiful generalization of the below mentioned formula.

P(A U B) = P(A) + P(B) - P(A \cap B)

A(A1 \cup A2 \cup A3) = P(A1) + P(A2) + P(A3) - (P(A \cap B) + P(A \cap C) + P(A \cap B)) + P(A \cap B \cap C)

But here is a formal derivation using indicator functions,

For a set A_i, we'll associate a indicator fn X_i which is 1 when outcome is inside and 0 when outcome is outside A_i

For A_i^c, indicator is 1 - X_i

A_i \cap A_j, indicator is X_i.X_j

A_i^c \cap A_j^c, indicator is (1 - X_i).(1 - X_j)

A_i \cup A_j, indicator is 1 - (1-X_i).(1-X_j)

Why? Because, A_i \cup A_j = (A_i^c \cap A_j^c)^c

Now,

P(A1 \cup A2 \cup A3) = E[indicator of A1 \cup A2 \cup A3]
= E[1 - (1 - X1)(1 - X2)(1 - X3)]
= E[1 - 1 + X1 + X2 + X3 - X1X2  - X2X3 - X1X3 + X1X2X3]

In general, this leads to the inclusion exclusion formula,

P(\cup_(k=1)^n A_k) = \Sigma_i P(A_i) - \Sigma_(i1 < i2)P(A_i1 \cap A_i2) + \Sigma_(i1 < i2 < i3)P(A_i1 \cap A_i2 \cap A_i3) - ... + (-1)^(n-1)P(\cap_(k=1)^n A_k)
*** Independence of events versus random variables
Let A and B be events.

X = I_A, Y = I_B.

X and Y are independent iff A and B are indenpendent. (and similarly for n events and indicator variables)
* Continuous random variables
** What is continuous R.V
f_X(x) is the probability density function for continuous r.v X.

  P(a \le X \le b) = \int_a^b f_X(x).dx

f_x(x)dx \ge 0, \int_(-\infin to \infin) f_X(x) = 1

Definition: A random variable is continuous if it can be described by a PDF.
i.e its not just enough if it a r.v takes values in a continues set, it also has
to have a PDF to describe it.

P(a \le X \le a + \delta) \approx f_X(a).\delta (integrals)
PDF is defined for intervals. Every point by itself has 0 probability.
But collectively, it has a positive values.

** Uniform continues R.V
\int_(a to b)c.dx = 1.

area = b.h = (b - a).h
Height of rectangle = 1/(b - a)
** Expected Value and Variance of continuous R.V
similar to the discrete case,

E[X] = \int_(-\infin to \infin)xf_X(x).dx

NOTE: This assumes that \int_(-\infin to \infin)|x|f_X(x).dx is well defined. i.e \le \infin

*** Expected Value rule
    E[g(X)] = \int_(-\infin to \infin) g(x)f_X(x).dx (derivation is little more complex)
*** Variance
var(X) = E[ (x - \mu)^2 ]

s.d \sigma_x = sqrt(var(X))
** Exponential R.V
parameter: \lambda > 0.
Exponential r.v usually models time until something happens.

f_X(x) = \lambda.e^(-\lambda.x) for x \ge 0

small \lambda, small initial value, small decay rate.
large \lambda, large initial value, but fast decay.

Very similar to Geometric R.V

P(X \ge a) = \int_(a to \infin)\lambda.e^(-\lambda.x)dx = e^(-\lambda.a)
E[X] = 1/\lambda
Var(X) = 1/\lambda^2

** Cumulative Distribution Function (CDF)
definition: F_X(x) = P(X \le x) = \int_(-\infin to x)f_X(x).dx

We can get PDF from CDF, dF_X(x)/dx = f_X(x)
** Normal R.Vs / Gaussian R.Vs
- Important in the theory of probability. (Central limit theorem)
- Convenient analytical properties.
- Model of noise consisting of many, small independent noise terms.


Standard Normal N(0, 1) = f_X(x) = 1/sqrt(2\pi) . e^(-x^2/2)
E[X] = 0
Var(X) = 1 (integration by parts)

General Normal N(\mu, \sigma) = 1/\sigma.sqrt(2\pi) . e^(-(x - \mu)^2/2.\sigma^2)

E[X] = \mu
Var(X) = \sigma^2

*** Linear functions of normal r.vs
Normal R.V is very useful analytically, cause of this property.

Given X ~ N(\mu, \sigma^2)

Y = a.X + b

E[Y] = a.\mu + b
Var(Y) = a^2.\sigma^2

Also, (we'll prove this later)

Y ~ N(a.\mu + b, a^2.\sigma^2)

Special case, a = 0, Y = b, lets think of this as the degenerate case where Y ~ N(b, 0)
*** Probabilities of Normal R.V
Unfortunately, there is no closed form solution available for CDF.
But we do have standard normal tables.

\Phi(Y) = F_Y(y) = P(Y \le y) = 'value from table'
**** convert any normal r.v to standard normal r.v

Let X be a normal variable and have mean \mu and variance \sigma^2.

Let Y = X - \mu/\sigma
E[Y] = 0.
Var(Y) = 1/\sigma^2 * Var(X) = 1.

Thus, Y is a standard normal variable.
** Conditional PDF

f_X|A(x).\delta \approx P(x \le X \le x + \delta|A)

The definition of conditional probability for continuous r.v

P(X \in B | A) = \int_(B)f_X|A(x).dx

\int f_X|A(x) = 1

Conditioning a c.r.v on an event,

P(x \le X \le x + \delta | X \in A) \approx f_(X|X \in A)(x).\delta

= P(x \le X \le x + \delta, X \in A)/P(A)
= P(x \le X \le x + \delta)/P(A) \approx f_X(x)/P(A)

E[X] = \int x.f_X(x)dx
E[X|A] = \int x.f_X|A(x)dx

Expected value rule,

E[g(x)] = \int g(x).f_X|A(x)dx
E[g(x)|A] = \int g(x).f_X|A(x)dx
** Memorylessness of exponential r.v
Do you prefer a new or used light bulb?

Bulb lifetime T: exponential in (\lambda)

P(T > x) = e^(-\lambda.x), for x \ge 0
- we are told t > T
- r.v X: remaining lifetime = T - t

P(X > x | T > t) = P(T-t > x, T > t) / P(T > t)
 = P(T > x + t, T > t)/P(T > t)
= P(T > x + t)/P(T > t)
= e^(-\lambda*(t + x))/e^(-\lambda.t) = e-^(\lambda.x)

using this memorylessness property, we can also see that

f_T(x) = \lambda.e^(-\lambda.x), for x \ge 0

P(0 \le T \le \delta) \approx \lambda.\delta

P(t \le T \le t + \delta | T > t) \approx \lambda.\delta

This is similar to independent coin flips, every \delta time steps,
with P(success) = \lambda.\delta
** Total Probability Theorem

F_X(x) = P(X \le x) = P(A_1).P(X \le x | A_1) + ...

= P(A_1).F_X|A_1(x) + ...

taking derivative on both sides,

f_X(x) = P(A_1).f_X(x) + ...

Multiplying by x and integrating

E[x] = P(A_1).E[ X|A_1 ] + ...
** Mixed distributions

Consider Y discrete, Z continuous as 2 random variables.

let X = { Y, with probability p; Z, with probability (1-p) }
X is mixed random variable.

F_X(x) = p.P(Y \le x) + (1-p).P(Z \le x) = p.F_Y(x) + (1-p).F_Z(x)
** Joint PDFs
Jointly continuous r.vs and joint pdfs

pdf is denoted by f_(X, Y)(x, y)

\int_(-\infin to \infin) \int_(-\infin to \infin) f_(X, Y)(x, y) = 1

Two r.v's are jointly continuous if they can be described by a joint pdf.

P(a \le X \le a + \delta, b \le Y \le b + \delta) \approx f_(X, Y)(a, c).\delta^2

f_(X, Y)(x, y): probability per unit area.

area(B) = 0 => P((X, Y) \in B) = 0

For joint continuity, its not enough if X and Y are continuous. We also need their probability to be spread over a two dimensional set.
** Joint to marginal PDFs

f_X(x) = \int f_(X, Y)(x, y).dy
f_Y(y) = \int f_(X, Y)(x, y).dx
** Joint CDF

we can get CDF from PDF by integrating,
F_X(x) = P(X \le x) = \int_(-\infin to x)f_X(t).dt

and conversely get PDF from CDF by differentiating,
f_X(x) = dF_X(x)/dx

F_(X, Y) = P(X \le x, Y \le y) = \int_(-\infin to y) \int_(-\infin to x)f_(X, Y)(s, t).ds.

If we take derivative, w.r.t y, and then with x, we get the joint pdf f_(X, Y)(s, t)

f_(X, Y)(x, y) = \delta^2.F_(X, Y)(x, y)/\delta.x\delta.y
** Conditional PDFs given another r.v

p_X|Y(x|y) = P(X = x|Y = y) = p_(X,Y)(x, y)/p_Y(y) if p_Y(y) > 0

However, in the continuous case p_Y(Y = y) = 0, so we can't use this definition, but
we can use this as a guide.

Definition:

f_X|Y(x|y) = f_(X, Y)(x, y)/f_Y(y) if f_Y(y) > 0 -------- 1

We know that the pdf/conditional pdf is used to approximate the probability of a small event

P(x \le X \le x + \delta | A) \approx f_X|A(x).\delta where P(A) > 0

the problem with using this to define conditioning on a continuous r.v is that
P_Y(Y = y) = 0. But we can instead define Y \approx y. y \le Y \le y + \epsilon

P(x \le X \le x + \delta | y \le Y \le y + \epsilon) \approx f_(X, Y)(x, y).\delta.\epsilon/f_Y(y).\epsilon

= f_X|Y(x | y).\delta

In general,


P(X \in A | Y = y) = \int_A f_X|Y(x|y)dx

Think of 1 as, the value of Y is fixed at some y.

\int_(-\infin to \infin)f_X|Y(x|y)dx = \int_(-\infin to \infin)f_(X, Y)(x, y)dx/f_Y(y) = 1

the f_Y(y) is needed so the the conditional probability integrates to 1. The numerator is the marginal pdf f_Y(y).
** Total probability and total expectation theorem
Theorem:

f_X(x) = \int_(-\infin to \infin)f_Y(y).f_X|Y(x|y).dy

definition:

E[X|Y] = \int_(-\infin to \infin)x.f_X|Y(x|y).dx

From this,

E[X] = \int_(-\infin to \infin)f_Y(y).E[X|Y].dy

Proof:
     = \int_(-\infin to \infin)f_Y(y).\int_(-\infin to \infin)(x.f_X|Y(x)).dx.dy
 = \int_(-\infin to \infin)x\int_(-\infin to \infin)f_Y(y)f_X|Y(y).dy.dx
= \int_(-\infin to \infin)x.f_X(x)dx.1
= E[X]
** Independence
Independence in the continuous r.v world is pretty similar to the discrete case,

If X & Y are independent,

f_(X, Y)(x, y) = f_X(x).f_Y(y) for all x & y.

equivalent to saying f_X|Y(x|y) = f_X(x) for all y with f_Y(y) > 0 or vice-versa.

Consequences of independence are also the same,

E[XY] = E[X].E[Y]

var(X + Y) = var(X) + var(Y)

g(X) & h(Y) are also independent: E[g(X)h(Y)] = E[g(X)].E[h(Y)]
** Stick breaking example
We have a stick of length l.

Break the stick twice,
- First break at X: uniform [0, l]
- Second break at Y: uniform [0, x]

f_(X, Y)(x, y) = f_X(x).f_Y|X(y) = 1/l * 1/x

f_Y(y) = \int_(-\infin to \infin)f_(X, Y)(x, y).dx = \int_(y to l)1/l.x dx = 1/l log(l/y)

E[Y] = \int_(0, l)1/l E[Y|X=x]dx = \int_(0, l)1/l * x/2 dx
** Independent normals
*** Independent standard normals
f_(X, Y)(x, y) = f_X(x).f_Y(y)
= 1/sqrt(2\pi) * exp(-x^2/2). 1/sqrt(2\pi) * exp(-y^2/2)

this can be rewritten as,

1/2\pi * exp(-1/2 (x^2 + y^2))
*** Independent general normals
f_(X, Y)(x, y) = f_X(x).f_Y(y)

= 1/2\pi(\sigma_X.\sigma_Y) * exp(- (x - \mu_x)^2/2\sigma_x^2 - (y - \mu_y)^2/2\sigma_y^2)

This is an equation that discribes an ellipse. It gives contours that are stretched along x and y based on the variances.
If X, Y are not independent, we get a bivariate normal distribution, that also has ellipses as contours, but are not parallel
to the x, y coordinates.
** Baye's rule variations
In baye's inference, we have a unobserverd value x, with prior p_X(.).
We observe y, with model p_Y|X(.|.). Then we infer p_X|Y(.|y)

p_X|Y(x|y) = p_Y|X(y|x).p(x)/p_Y(y)

In a continuous setting, its very similar.

f_X|Y(x/y) = f_Y(x).f_Y|X(y|x)/f_Y(y)
*** Mixed baye's rule
Dealing with discrete and continuous r.v.

K: discrete, Y: continuous

p(K = k, y \le Y \le y + \delta)
= p_K(k).p(y \le Y \le y + \delta|K=k) = p_K(k) * f_Y|K(y).\delta
= p(y \le Y \le y + \delta).p(k/y \le Y \le y + \delta) = f_Y(y).\delta * p_K|Y(k|y)

Thus, rearranging, we have

p_K|Y(k|y) = p_K(k).f_Y|K(y|k) / f_Y(y)

f_Y(y) = \Sigma_K p_K(k').f_Y|K(y|k')

 OR

f_Y|K(y | k) = p_K(k | y).f_Y(y)/p_K(k)
p_K(k) = \int f_Y(y').p_K|Y(k|y')dy'
* Further topics on random variables
** Derived Distributions
*** PMF of a general function of a discrete R.V
Let X be an R.V and Y = g(X)
p_Y(y) = \Sigma_(g(x) = y)p_X(x)
**** Linear function of discrete r.v

Y = a.X + b

p_Y(Y=y) = p_X(X=y - b/a)
*** Linear function of continous r.v
In case of a continuous r.v, let X be a continous r.v and Y = a.X + b

If a = 0, Y is a constant r.v

when a > 0,

p(Y = y) = p(Y = a.X + b) = p(X = y - b /a) = 0

This is because in the continous case, probability of a point is 0.

Instead, lets work with intervals, by using CDFs

F_Y(y) = P(Y \le y) = P(a.X + b \le y) = P(X \le (y - b)/a) = F_X((y - b)/a)

To find the pdf, lets differentiate.

f_Y(y) = f_X((y - b)/a)*1/a

when a < 0,

we get F_Y(y) = P(X \ge (y - b)/a) = 1 - P(X \le (y - b)/a)

Using chain rule again,

f_Y(y) = -1/a * f_X(y - b/a)

Thus,

f_Y(y) = 1/|a| * f_X((y - b)/a)
*** Linear function of normal r.v

Let X ~ N(\mu, \sigma^2)

f_X(x) = 1/sqrt(2\pi) * exp{-(x - \mu)^2/2.\sigma^2}

Y = a.X + b

From above, we know that f_Y(y) = 1/|a| * f_X((y - b)/ a)

Substituing, x as (y - b /a), we get

Y = aX + b ~ N(a\mu + b, a^2.\sigma^2)
*** PDF of a general function of a continous r.v
Its a two step procedure,

- find the CDF. F_Y(y) = P(Y \le y) = P(g(X) \le  y)
- differentiate it.
**** Ex 1
Y = X^3, X is uniform on [0, 2]

Y goes from [0, 8]

F_Y(y) = P(X^3 \le y) = P(X \le y^(1/3)) = 1/2 * y^(1/3)

f_Y(y) = 1/6 * y^(1/3)
**** Ex 2
X is uniform on [5, 10]
Y = 10/X

Y takes values between [1, 2]

F_Y(y) = P(Y \le y) = P(10/X \le y) = P(X \ge 10/y) = 1/5 * (10 - 10/y)

f_Y(y) = 2/y^2 1 \le y \le 2
**** General formula for the monotonic function case
Y = g(X), When g is monotic, there turns out to be a general formula for computing the PDF.

what is monotic, x < x' => g(x) < g(x)'.

Assuming g is strictly increasing and differentiable,
In this case the inverse function h, gives us x for a given value of y. x = h(y)

F_Y(y) = P(Y \le y) = P(X \le h(y)) = F_X(h(y))

Thus,

f_Y(y) = f_X(h(y)). d(h(y))/dh

If g is a strictly deacreasing function of h,

F_Y(y) = P(Y \le y) = P(X \ge h(y)) = 1 - F_X(h(y))

Thus,
f_Y(y) = -f_X(h(y)).d(h(y))/dy.

h is increasing/decreasing just as g. Thus, we can use absolute value for dh/dy

Thus, in both cases,

f_Y(y) = f_X(h(y)).|d(h(y)/dy|
***** Intuitive derivation for the monotonic function case

P(y \le Y \le y + \delta_1) = P(x \le X \le x + \delta_2)

P(x \le X \le x + \delta_2) \approx f_X(x).\delta_2
P(y \le Y \le x + \delta_1) \approx f_Y(y).\delta_1

\delta_2 = \delta_1 * d(h(x)/dy (i.e how much did x wiggle, given that y wiggled by \delta_1 :) )

f_X(x).\delta_1 * d(h(x))/dy = f_Y(y).\delta_1

We get the same result, f_Y(y) = f_X(x) * d(h(x))/dy
**** Nonmonotonic case
Ex) Y = X^2


F_Y(y) = P_Y(Y \le y) = P_X(X^2 \le y) = P_X(|X| \le sqrt(y)) = P_X(-sqrt(y) \le X \le sqrt(y))

= F_X(sqrt(y)) - F_X(-sqrt(y))

f_Y(y) = f_X(sqrt(y)).1/2.sqrt(y) + f_X(-sqrt(y)).1/2.sqrt(y)
**** A function of multiple r.vs

Z = g(X, Y)

Same method, find CDF(Z) and then find PDF(Z)
** Distribution of sums of independent r.vs
*** Sum of independent discrete r.vs
Z = X + Y; X, Y independent with known pmfs, discrete

As x, y are independent P_(X, Y)(x, y) = P_X(x).P_Y(y)

P_Z(z) = \Sigma_x P(X=x, Y=y)
      = \Sigma_x P(X=x, Y=z - x)

This is called the *convolution formula*.

Ex)
X and Y are independent,

Let X = {1, 2, 3} with probability {1/6, 3/6, 2/6}
Let Y = {2, 3, 4} with probability {2/6, 3/6, 1/6}

Z = X + Y

p_Z(6) = {2, 4} + {3, 3}

But figuring this out by inspection can get painful.

Instead, this is easier done using the convolution formula.

y = z - x

flip Y, we get {-2, -3, -4} with probability {2/6, 3/6, 1/6}

shift by (add) z=6

flipped and shifted Y: {4, 3, 2} with probability {2/6, 3/6, 1/6}

Now, multiply the probabilities that are on top of each other.

1/6 * 3/6 + 3/6 * 2/6 = 3/36 + 6/36 = 9/36 = 1/4
*** Sum of independent continuous r.vs
continuous convolution formula:

f_Z(z) = \int_(-\infin to \infin)f_X(x).f_Y(z - x)dx

How?

Let Z = X + Y

Given x = 3, z = y + 3

f_Z|X(z|3) = f_(Y + 3)(z|3) = f_(Y+3|X)(z|3) = f_(Y + 3)(z) as X and Y are independent.

f_(X + b)(x) = f_X(x - b)

Thus, f_(Y + 3)(z) = f_Y(z - 3)

In general,

f_(Z|X)(z|x) = f_Y(z - x)

Joint PDF of X and Z

f_(X, Y)(x, z) = f_X(x).f_Y(z - x)

From joint to marginal,

f_Z(z) = \int_(-\infin to \infin)f_(X, Z)(x, z)dx = \int_(-\infin to \infin)f_X(x).f_Y(z - x)dx

Same mechanics as discrete case (flip and shift)
*** Sum of independent normal r.vs
Doing some algebra, the sum of finitely many independent normal r.vs is also normal

X ~ N(\mu_X, \sigma^2_x), Y ~ N(\mu_Y, \sigma^2_y), Z ~ N(\mu_Z, \sigma^2_z)

Then X + Y + Z ~ N(\mu_X + \mu_Y + \mu_Z, \sigma_X^2 + \sigma_Y^2 + \sigma_Z^2)
** Covariance
Let X, Y be 2 r.vs
In general, covariance helps us know if 2 r.vs typically tend to go together.

Covariance > 0, positive values of x, y go together, and -ve values go together
Covariance < 0, positive values of x tends to go with negative y and vice-versa.

Definition,

cov(X, Y) = E[(X - E[X]).(Y - E[Y])]

Now, if X, Y are independent

cov(X, Y) = E[(X - E[X])].E[(Y - E[Y])] = 0.

Thus, independence => cov(X, Y) is 0.
But the converse is not true.
*** Properties
- cov(X, X) = E[ (X - E[X])^2 ] = var(X)
**** Alternate formula for covariance
E[XY + E[X].E[Y] -YE[X] - XE[Y]]

E[XY] + E[X].E[Y] - E[Y].E[X] - E[X].E[Y]

Thus, cov(X, Y) = E[XY] - E[X].E[Y]
**** Covariance of linear function of a value
Assume 0 means, (for simpler calculation)

cov(X, a.Y + b) = E[aXY + bX] = aE[XY] + bE[X] = a.cov(XY)

Thus, multiplying by a constant increases covariance by a, but adding constant does not
affect it.

cov(X, Y + Z) = E[X.Y + X.Z] = cov(X, Y) + cov(X, Z)
** Variance of sum of r.vs
   
var(X1 + X2) = E[((X1 + X2) - E[X1 + X2])^2 ]

Expanding and grouping,

= E[(X1 - E[X1])^2 + (X2 - E[X2])^2 + 2.(X1 - E[X1]).(X2 - E[X2])]
= var(X1) + var(X2) + 2.cov(X1, X2)

Expanding this for multiple r.vs

Assume 0 mean for all r.vs

var(X1 + X2 + ... Xn) = E[ (X1 + X2 + ...X_n)^2 ]
  = E[ \Sigma_n(X_i)^2 + \Sigma_(i \ne j)X_i.X_j ]
  = \Sigma_n var(X_i) + \Sigma_(i \ne j)cov(X_i, X_j)
** Correlation
Dimensionless version of the covariance.

  \rho(X, Y) = cov(X, Y)/\sigma_x.\sigma_y

It measures the degree of association between X and Y, but has fixed upper and lower limits.

*** Properties
**** \rho always lies between -1 and +1
For simplicity, lets assume 0 mean and unit variance. (but its true for all cases)

E[(X - \rho.Y)^2 ] = 1 - 2*\rho^2 + \rho^2 = 1 - \rho^2

Since E[(X - \rho.Y)^2) should be \ge 0.
Thus 1 - \rho^2 \ge 0
\rho^2 \le 1 or -1 \le \rho \le 1
**** Independent r.vs => 0 correlation. (but converse is not true)
**** \rho(X, X) = 1, \rho(X, -X) = -1.
**** If |\rho| = 1 <=> (X - E[X]) = c.(Y - E[Y])  (linearly related)
**** cov(a.X + b, Y) = a.cov(X, Y) => \rho(aX + b, Y) = a.cov(X, Y)/|a|.\sigma_X.\sigma_y = sign(a) * \rho(X, Y)
Thus, going from one set of units to another does not affect the association with some other variable.
This reflects in the dimensionless property of the correlation.
*** Interpretation of the correlation
A large correlation implies Association but not causation.

What does a large correlation mean?

There is some underlying common (but perhaps hidden) factor that affects both the variables.
*** Correlations matter, a nice problem

Say we have $100M  and decide to invest $10M in each of 10 states.
At each state i, the return on investment is a random variable X_i, with mean 1 and s.d 1.3 (in millions).
Should we invest or not?

var(X_1 + X_2 + ... X_10) = \Sigma_(i=1 to 10) var(X_i) + \Sigma_((i, j): i \ne j)cov(X_i, X_j)

Now, if (X_i)'s  are uncorrelated, then  var = 10 * (1.3)^2 = 16.9, thus \sigma = 4.1
Expected return = 10.
Thus, in order to make a loss, we have to go like about 2.5 s.ds below the mean.

Using chebyshevs condition, the proportion of entires greater than 2.5 s.ds is <= 0.16

However, if the (X_i)'s are correlated (say \rho = 0.9)
Then cov(X_i, X_j) = \rho.\sigma_x.\sigma_y = 1.52
Then, variance = 16.9 + 90 * 1.52 = 154.
\sigma(X_1 + X_2 + ... X_10) = 12.4.

Hmmm, event one s.d below can lead to losses, so its a nono.
** Conditional expectation and variance as r.vs
*** Conditional expectation as an r.v

Given X is an r.v and h(x) = x^2.
What is h(X)?
This is a random variable, that takes the value x^2 when X takes the value x.

We know,

E[X|Y=y] = \Sigma_x x.p_X|Y(x|y) (integration in the continuous case)

Lets call this g(y). (Its a value)

now, what is g(Y)?
Its a r.v that takes the value E[X|Y=y] when Y takes the value y.

Definition, E[X|Y] = g(Y)
- Its a function of Y
- It is a r.v
- has a distribution mean, variance etc...
**** Whats its mean?

E[E[X|Y]] = E[X]

This is called the law of iterated expectations.

Using the expected value rule,
E[g(Y)] = \Sigma_y g(Y=y).p_Y(y) = \Sigma_y E[X|Y=y]*p_Y(y)

From the total expectation theorem, we get E[X]
**** Forecast revision example
Suppose forecasts are made by calculating the expected value, given any information.

X: February sales

Forecasting in the begining of the year: E[X]

End of january: will get new information, Y=y.

New forecast: E[X|Y=y]

But at the begining of the year, we don't know the revised forecast. its a r.v E[X|Y]

Using law of iterated expectations, we get E[E[X|Y]] = E[X]
i.e expected value of revised foreecast is the same as the original expectation.
*** Conditional variance as an r.v
var(X) = E[(X - E[X])^2)

var(X|Y=y) = E[(X - E[X|Y=y])^2 |Y=y] (this is a value)

Similar to the conditional expectation case, we can write a r.v as

var(X|Y) = E[(X - E[X|Y])^2 |Y]

var(X|Y) takes the value var(X|Y=y) when Y=y

**** What is its expected value?
var(X|Y=y) = E[X^2 |Y=y] - E[X|Y=y]^2 for all y (this is a value)

var(X|Y) = E[X^2 |Y] - E[X|Y]^2 (this is an r.v)

E[var(X|Y)] = E[X^2 ] - E[E[X|Y]^2 ] (using law of iterated expectation for the first term)
**** Law of total variance
var(X) = E[var(X|Y)] + var(E[X|Y])

the proof is just manipulation, no intuition

E[var(X|Y)] = E[X^2 ] - E[E[X|Y]^2 ] (using law of iterated expectation for the first term)

var(E[X|Y]) = E[E[X|Y])^2 ] - E[X]^2

Thus, E[var(X|Y)] + var(E[X|Y]) = E[X^2 |Y] - E[X^2 ] = var(X)
**** Example: Section mean and variance

Two sections of a class: y=1 (10 students), y=2 (20 students)
x_i: score of student i

Experiment: Pick a stundent at random (uniformly)

random variables: X and Y where X takes the score of the selected student and Y is the section.

Given data,

E[X|Y=1] = 90, E[X|Y=2] = 60


We can find E[X]

E[X] = \Sigma_x x_i.p(x_i) = 1/30 * \Sigma_(i=1 to 30)(x_i) = 1/30 * (90 * 10 + 60 * 20) = 70

***** E[X|Y] ?

1/3 * 90 + 2/3 * 60 = 70 which is the same as E[X]. cool!

Thus, we can find E[X] using this new divide and conquer method using law of iterated expectations.
***** var(E[X|Y])?
E[(E[X|Y] - E[E[X|Y]])^2 ] = 1/3 * (90 - 70)^2 + 2/3 * (60 - 70)^2 = 200
***** var(X|Y=y)
var(X|Y=1) = E[(X - E[X|Y=1])^2 |Y=1 ] =  from above, we can see that this is 10

Similarly

var(X|Y=2) = 20
***** E[var(X|Y)]
1/3 * 10 + 2/3 * 20 = 50/3
***** var(X)
E[var(X|Y)] + var(E[X|Y])

(average variability within a section) + (variability between sections)

i.e the overall randomness can be broken into these two pieces.
**** Mean of the sum of a random number of random variables
N: number of stores visited (N is a nonnegative integer r.v)
X_i: money spent at store i
- X_i independent, identically distributed
- independent of N

Total money spent Y = X_1 + X_2 ... + X_N

hmmm, how do we do this?

always a good idea to condition on something and then see if we can figure stuff out.

E[Y|N=n] = E[X_1 + ... X_n | N=n] = E[ X_1 ] + E[ X_2 ] + ... E[ X_n ] (as X_i and N are independent)
Let each E[X_i ] = E[X]
Thus E[Y|N=n] = n.E[X]

Using total expectation theorem,

E[Y] = \Sigma_y p_N(n)E[Y|N=n] = \Sigma_n P_N(n).n.E[X] = E[N].E[X]
which makes intuitive sense. Its the expected number of stores visited times the money spent at each store.

Another approach is to use the law of iterated expectations,

E[Y] = E[E[Y|N]] = E[N.E[X]] = E[X].E[N] (As E[X] is a constant)
**** Variance of the sum of a random number of random variables
var(Y) = ?

Y = X_1 + ... X_N

from the law of total variance,

we know var(Y) = E[var(Y|N)] + var(E[Y|N])

E[Y|N] = N.E[X]

var(E[Y|N]) = var(N.E[X]) = E[X]^2.var(N)

to find the first term,
var(Y|N=n) = var(X_1 + ... X_n | N=n) = var(X_1 + ... X_n) = n.var(X)

var(Y|N) = N.var(X)
E[var(Y|N)] = E[N.var(X)] = var(X).E[N]

Thus, var(Y) = var(X).E[N] + E[X]^2.var(N)
* Bayesian Inference

If our probability model is good, it should be able to predict the real world.
Using Data from the real world, we do inference and statistics to improve our model.

Application domains:

Polls, marketing/advertising, finance, natural sciences, neuroscience, astronomy, engineering (fight against noise).
** Types of Inference problems
*** Model building versus inferring unobserved variables
Ex) X = a.S + W

S is sent, W is noise.

Inference: we know X, we know a, we infer X.
Model building: we know S, we observe X, we find a.

Usually in inference problems, we try to estimate some numerical unknown value. (Try to get as close to it as we can)
*** Hypothesis testing versus estimation
- unknown takes one of few possible values.
- make the probability of incorrect decision pretty small.
** Introduction to Bayesian inference framework
Unknown \theta
- treated as an r.v
- prior distribution p_\Theta or f_\Theta

Observation X
- observation model p_X|\theta or f_X|\theta

Use appropriate version of the Bayes rule to find p_\theta|X(.|X = x) or f_\theta|X(.|X = x)

/Where does the initial prior come from?/
symmetry, known ranges, earlier studies, subjective or arbitrary.

The complete answer in a bayesian inference problem is the posterior distribution: PMF p_\theta|X(. | x) or PDF f_\theta|X(. | x)
*** Point estimates in Bayesian Inference
If suppose we want one answer to describe the posterior, what would it be?

2 ways:
- Maximum a posteriori probability (MAP):

p_\theta|X(\theta^* | x) = max_\theta p_\theta|X(\theta|x)
f_\theta|X(\theta^* | x) = max_\theta f_\theta|X(\theta|x)

the value of \theta that maximizes the conditional probability

- Conditional expectation: E[\theta|X=x] (LMS: least mean square)

Note:
estimate: \theta = g(x) (number)
estimator: \Theta = g(X) (r.v)
*** Discreate parameter \Theta, discrete X
*** Discrete parameter \Theta, continuous observation

Both cases above are handled by a simple application of the bayes rule. (using pmf or pdf as required)

p_(\Theta|x)(\theta|x) = p_\Theta(\theta) * f_X|\Theta(x|\theta) / f_X(x)

where f_X(x) = \Sigma_\theta' f_X|\Theta(x|\theta') * p(\theta')

Overall probability of error:

P(\Theta' \ne \theta) = \Sigma_\theta P(\theta' \ne \theta | \theta = \theta) * p_\theta(\theta)
** Linear models with normal noise
*** Recognizing normal pdfs


X ~ N(\mu, \sigma^2) f_X(x) = 1/\sigma*sqrt(2\pi) * e^-(x - \mu)^2/2\sigma^2

f_X(x) = c.e^(-(\alpha*x^2 + \beta * x + \gamma)  \alpha > 0 as the pdf has to integrate to 1.

We can use the trick of 'completing the square' to convert this to the form c.e^(-(x - \mu)/2*\sigma^2) (Normal distribution form)

\alpha(x^2 + \beta * x/\alpha + \gamma/\alpha) = \alpha((x + \beta/2.\alpha)^2 -\beta^2/4.\alpha^2 + \gamma/\alpha

Thus, we can write

f_X(x) = c.e^(-\alpha* (x + \beta/2*\alpha)^2).e^(-\alpha*(-\beta^2/4.\alpha^2 + \gamma/\alpha))

Which is of the form  N(-\beta/2*\alpha, 1/2.\alpha)

Once we are convinced this is a normal fn, To find the peak (which is also the mean, as this is a normal distribution)
we need not do this completion of squares thingy, instead just maximize the function (or minimize the quadratic exponent)

2 * \alpha * x + \beta = 0

x = -\beta/2.\alpha (which is the mean/peak)

Thus exponential of a quadratic function of x is a normal pdf.
*** Estimating a normal r.v in the presence of additive normal noise

X = \Theta + W       \Theta, W ~ N(0, 1) and independent

Now how do we infer \theta?

we have to do that using bayes inference.

f_(\Theta/x)(\theta/x) = f_(x/\theta)(x/\theta) * f(\theta) /f_X (x)

Since X = \theta + W, its distribution is N(\theta, 1)

Thus, f_(\theta|X)(\theta|x) = 1/f_X(x) * c * e^(-1/2 * \theta^2).c * e^(-1/2 * (x - \theta)^2 = c(x).e^(- quadratic)

Thus, we observe that the posterior is normal.

Hence \Theta_Map = \Theta_LMS = E[\Theta|X=x] = Mean or Peak of the posterior distribution.

We can find the peak by minimizing the quadratatic. (Which happens to be x/2)

The estimator \Theta_MAP = X/2

Even with general means and variances:
- posterior is normal.
- LMS and MAP estimators coincide.
- these estimators are linear, of the form \Theta = a.X + b
*** Case of multiple observations
This is a very interesting model that appears quite often in practice.
We observe the same variable multiple times and then try to estimate the variable.

X_1 = \Theta + W_1
.
.
.
X_N = \Theta + W_N

\Theta ~ N(x_0, \sigma_0^2) W_i ~ N(0, \sigma_i^2)

\Theta, W_1 ... W_N are independent.

Let x = [x_1, x_2, ... x_N ], i.e a vector of observations.

Now given this vector, whats our estimate of \theta? We can use bayes rules f_\Theta|X(\theta|x)

f_(X_i | \theta) = c_i.e^(-(x_i - \mu)/\sigma_i^2)

f_(X|\Theta)(x|\theta) = f_(X_1, ... X_N|\Theta)(x_1, ... x_n | \Theta)

given \Theta = \theta: W_i 's are independent (knowing \theta does not make them dependent)
Now, X_i 's are obtained by just adding a constant to these w_i 's, hence they are independent.

Thus, we can write f_(X|\Theta)(x|\theta) = \prod_i f_(X_i | \theta) (x_i | \theta)

f_(\Theta|x) (\theta|x) = 1/f_X(x) * c_0.e^(-(\theta - x_0)^2/2.\sigma_0^2) * \prod_i c_i.e^(-(x_i - \theta)^2/2.\sigma_i^2)

Rearranging and combining terms, we recognize this as a normal distribution.

Find the peak:

We take derivative w.r.t \theta and set to 0.

\Sigma_(i = 0 to n) (\theta - x_i) / \sigma_i^2 = 0

We get \theta = \Sigma_(i = 0 to n) x_i/\sigma_i^2  / \Sigma_(i = 0 to n) 1/\sigma_i^2

**** Key conclusions
- posterior is normal
- LMS and MAP estimates coincide.
- these estimates are 'linear', of the form \theta = a_0 + a_1.x_1 + ... + a_n.x_n

Interpretations:

- estimate \theta: weighted average of x_0 (prior mean) and x_i (observations)
- weights determined by variances.
*** Mean squared error
Performance measures for our estimate.

- Mean squared error:
After estimating \theta', we find the mean squared error as follows.

E[(\theta - \theta')^2 |X = x] = var(\theta | X  = x) (as our estimate \theta is the mean value)

Using the formula for estimating variance of a normal (with quadratic in the exponent),

we get var(\theta|X = x) = 1/\Sigma_(i = 0 to n)1/\sigma_i^2

Thus, thats our mean squared error.
**** Example
X = \Theta + W

\Theta ~ N(0, 1), W ~ N(0, 1) independent \Theta, W

We estimated \Theta' = X/2  E[(\Theta - \Theta')^2 | X = x] = prior variance + variance of observation = 1/2 (using our formula above)
*** Multiple parameter, trajectory estimation
Seen a lot in the real world.

Lets take an example,

Someone throws a ball along a projectile.

From newtons laws,

x(t) = \theta_0 + \theta_1.t + \theta_2.t^2

- We don't know the \theta's, so lets model them as random variables \Theta_0, \Theta_1, \Theta_2.
Independent priors, f_\theta_j

- We take measurements at times t_1, ... t_n

X_i = \Theta_0 + \Theta_1.t_i + \theta_2.t_i^2 + W_i

noise model, f_W_i.
Independent W_i, independent from \theta_j.

A Model with normality assumptions.

Assume \Theta_j ~ N(0, \sigma_j), W_i ~ N(0, \sigma^2); independent.

- Given \Theta = (\theta_0, \theta_1, \theta_2); X_i ~ N(\theta_0 + \theta_1.t_i + \theta_2.t_i^2, \sigma_i^2)

posterior = 1/f_X(x) * \prod_(j = to 2) f_\theta_j(\theta_j). \prod_(i = 1 to n) f_X_j|\theta(x_i | \theta)

MAP estimate: maximize over (\theta_0, \theta_1, \theta_2) (minimize the quadratic)

we get 3 equations (linear), 3 unknowns.
*** Linear normal models
- \Theta_j and X_i are linear functions of independent normal random variables.

Inference under this class of models goes under the name linear regression.

we do this just like the trajectory estimation we did above.

- f_\Theta|X(\theta|x) = c(x).exp { -quadratic(\theta_1, ... \theta_m) }
- MAP estimate: maximize over (\theta_1, ... \theta_m);
(minimize quadratic equation, just by taking derivative and setting to 0)

\Theta'_(MAP, j): linear function of X = (X_1, ... X_n).

Facts:
- \theta'_(MAP, j) = E[\Theta_j | X]
- marginal posterior PDF of \Theta_j: f_(\theta_j | X) (\theta_j | x), is normal
- MAP estimates based on the joint posterior PDF: same as MAP estimate based on marginal posterior PDF.
- E[(\Theta'_(i, MAP) - \Theta_i)^2 | X = x]: same for all x.

Thus, this class of models have a rich set of important and elegant properties.
Hence, they are used very much in practice. (probably the most used statistical models)

/note/ this model goes under the name of linear regression.
To be precise, this model is mathematically equivalent to a (Bayesian) linear regression model.
However, in typical presentations of linear regression, the notation and the interpretation
(in terms of "explanatory" and "dependent" variables) looks quite different at first sight.
** Least mean square estimation

So far, we have used 2 types of estimates, 1) MAP 2) Mean of conditional expectation.
*** How about using some criterion and then minimizing that?
Minimize the mean squared error. E[(\Theta - \theta')^2 | X = x].
This is called the *least mean square estimator*
*** LMS without any observations
unknown \theta; prior p_\theta(\theta)

Lets take an example prior: uniform [4, 10]
- no observations available.
- MAP rule: find the point where this distribution is the highest.
In this case, its any \theta \in [4, 10]
- (Conditional) expectation
In this case, its 7.

But which is the 'right' one?

Lets use a criterion, mean squared error. E[(\theta - \theta')^2 ] where \theta' is our estimate (constant)

Minimize this,

Expanding and setting derivative to 0,

E[ \theta^2 ] + \theta'^2 - 2.\theta'.E[\theta] = 0

d/d\theta'

\theta' =  E[\theta]

Thus, the optimal estimate according to LMS is the expected value.

Another way to show this (more from a probability perspective, and less calculusy...)

var(\theta - \theta') + (E[\theta - \theta']^2)

var(\theta) + (E[\theta - \theta']^2)
We can make the expected value 0 when \theta' = E[\theta]

Optimal mean squared error = E[ (\theta - E[\theta])^2 ] = var(\theta)
*** LMS single unknown and observation
unknown \theta, prior p_\theta(\theta)
- interested in a point estimate \theta'.

observation X; model p_X|\theta(x|\theta)
- observe that X = x

minimize the conditional mean squared error

E[(\theta - \theta')^2 | X=x]

The minimization process is the same as before, except that this is in a conditional universe.

\theta' = E[\theta|X=x]

Estimator function is E[\theta|X]
*** LMS performance evaluation
How good is our estimate?
MSE = E[(\Theta - E[\theta|X=x])^2 | X = x] = var(\Theta|X=x)

Thus, conditional variance is the optimal mean squared error.

Expected performance of design, (we haven't used our estimate, so no  observation)

MSE = E[(\Theta - E[\theta|X])^2 ] = E[var(\Theta|X)]
*** LMS estimation of \Theta based on X
- LMS relevant to estimation (not hypothesis testing).
- Same as MAP if the posterior is unimodal and symmetric around the mean.
*** Multidimensional case
LMS estimation with multiple observations and unknowns.

unknown: \theta; prior: p_\theta(\theta)
interested in point estimate \theta'

Observations X = (X1, X2, X3 ... ): model p_X(x|\theta)

We previously did not rely on the dimension of X.

LMS estimate: E[\theta | X1=x_1, X2=x_2 ... ]

If \Theta is a vector, apply to each component seperately.

\Theta = (\theta_1, ... \theta_m)

then \theta'_j = E[\Theta_j | X1 = x_1, X2 = x_2 ... ]
*** Challenges of LMS estimation

By bayes rule,

f_\Theta(\theta|X) = f_(X|\theta)(x|\theta).f_\theta(\theta) / f_X(x)

f_X(x) = \int f_\theta(\theta').f_X|\theta(x|\theta')d\theta'

- Full correct model, f_X|\theta(x|\theta), may not be available.
- can be hard to compute/implement/analyze.
*** Properties of LMS estimation
Estimator \Theta' = E[\Theta|X]
Error = \Theta' - \theta


E[error|X=x] = 0

proof:

E[estimate - \theta|X=x] = estimate - E[\Theta|X=x] = 0

cov(error, estimator) = 0

proof:
cov(error, estimate) = E[error * estimate] - E[error].E[estimate]

Conditioning on X,

E[error * estimate | X = x] = estimate * E[error|X] = 0

the second term in cov is 0, as E[error|X] = 0

\theta = estimate - error

var(Estimator) = var(estimator) + var(error) (property of variance)

Thus, variance can be composed into 2 pieces, variance of the estimator and the variance of the estimation error.
** Linear least mean square estimation

Conditional expectation E[\theta|X] may be hard to compute/implement.
estrict the estimator \theta' = aX + b.
*** LLMS formulation

Unknown \theta; observation X

- Minimize E[ (\theta - \theta')^2 ]
- Estimator \theta' = g(X) -> \theta'_LMS = E[\theta|X]
- Now instead, consider estimators of the form \theta' = a.X + b

Minimizing E[ (\theta - aX - b)^2 ], w.r.t a, b.

*note* If E[\theta|X] is linear, then \theta'_LMS = \theta'_LLMS
*** Solution to the LLMS formulation

minimize E[ (\theta - a.X - b)^2 ], w.r.t a, b.

- suppose a has already been found: b = E[\theta] - a.E[X]. (Same argument as before, where we found \theta'=E[\theta] for E[(\theta - \theta')^2 ].

substituting our b,

min E[ (\theta - a.X) - E[\theta - a.X])^2 ] = var(\theta - a.X)

var(\theta - a.X) = var(\theta) + a^2.var(X) - 2.a.cov(\theta, X)

d/d\theta = 0: a = cov(\theta, X)/var(X)

Thus,

\theta'_LLMS = E[\theta] + Cov(\theta, X)/var(X)(X - E[X])

Alternatively,
\rho = cov(\theta, X)/\sigma_\theta.\sigma_x

a = \rho.\sigma_\theta.\sigma_x / \sigma_X^2

\theta_LLMS ' = E[\theta] + \rho.\sigma_\theta/\sigma_X(X - E[X])
*** Properties
- Only means, variances and covariances matter.
- \rho > 0:
If we see X > E[\theta] => \theta'_LLMS > E[\theta]
- Similarly if \rho < 0, when we see large X, we'll comeup with low estimate for \theta.
- \rho = 0:
\theta'_LLMS = E[\theta]

Which also makes sense, if there is no correlation, it just reports the expected value.
*** MSE for LLMS
To simplify algebra, we'll work with 0 mean and variance case. (though the result holds for all cases)

E[ (\theta - \rho.\sigma_\theta/\sigma_X.X)^2 ] = (1 - \rho^2).var(\theta)

Thus, large low implies lower mean square estimation error.
*** Coin bias example
- coin with bias \theta; prior f_\theta(.)
- fix n; X = number of heads

Find the bias.

Let f_\theta(.) be uniform [0, 1]

\theta_LMS = X+1/ n+2

lets derive this from LLMS, (as the LMS is linear, LLMS should give us the same result)

E[\theta] = 1/2 var(\theta) = 1/12

p_X|\theta: Bin(n, \theta) E[X|\theta] = n.\theta var(X|\theta) = n.\theta(1-\theta)

E[X] = E[n\theta] = n/2 (using law of iterated expectation on E[X|\theta])

E[X^2 | \theta] = var(X|\Theta) + E[X^2 |\Theta]^2 = E[n.\theta(1-\theta) + n^2.\theta^2 ] = E[n\theta - + (n^2 - n).\theta^2 ]

E[ X^2 ] = E[E[X^2 | \theta]] = n/2 + (n^2 - n)/3 = n/6 + n^2/3

var(X) = E[ X^2 ]  - E[X]^2 = n/6 + n^2/3 - n^2/4 = n(n + 2)/12

cov(\theta/X) = n/3 - n/4 = n/12

Using this stuff,

\Theta_LLMS = X + 1 / n+2
*** LLMS with multiple observations
- Unknown \Theta; observations X = (X1, X2, X3 ... Xn)
- Consider estimators of the form: \theta' = a_1.X_1 + ... + a_n.X_n + b
- Find the best choices of a1, a2, ... an

minimize E[ (a_1.X_1 + ... + a_n.X_n + b - \theta)^2 ]

- If E[\theta|X] is linear in X, then \Theta'_LMS = \theta'_LLMS
**** How to minimize this?
Take derivative and set to 0.

Taking derivative of quadrative will give us a linear equation in b and the a_i

- Only means, variances and covariances matter.
- If multiple unknown \theta_j, apply to each one seperately.
*** Representation of data matters
**** Estimation based on X versus X^3
- LMS: E[\theta|X] is the same as E[ \theta|X^3 ]
- LLMS is different: estimator \theta' = a.X + b versus \theta' = a.X^3 + b

For finding a.X^3 + b, we need to know cov(X^3, \theta) var(X^3)
Generally, these higher order r.vs are harder to compute.

We can take this further,

\theta' = a1.X + a2.X^2 + a3.X^3 + ... + b
\theta' = a1.X + a2.e^X + ... + b

*Note* these estimators are still called linear estimators, as our coefficients are linear.

These are harder to compute, but are useful when we know somewhat know the shape of the output.
* Limit theorems and classical statistics
** Inequalities
Generally, in probability, Inequalities use a bit of information about
a distribution to learn something about probabilities of 'extreme
events'.
*** Markov Inequality
Intuitively, If X \ge 0, and E[X] is small, then X is unlikely to be very large.

If X \ge 0 and a > 0, then P(X \ge a) \le E[X]/a.

Thus, if the expected value is small, this probability is also small.
If a is very large, again this probability is small.

Another way to think about this,

If X \ge 0, and k is a constant, then P(X \ge k.E[X]) \le 1/k
**** Proof

E[X] = \int_(0 to \infin) x.f(x)dx

P(X \ge a) = \int_(a to \infin)x.f(x)dx

Since E[X] is an integral over a larger region of the same function,

E[X] \ge P(X \ge a)
     \ge \int_(a to \infin)a.f(x).dx (substituting the lower bound a, for x)
     \ge a.P(X \ge a)

Here is an instructive proof of the same,

Let Y = { 0 if X < a, a if X \ge a}

Y \le X (from the above definition)

Thus, E[Y] \le E[X]

a.P(X \ge a) \le E[X]
*** Chebyshev Inequality
Consider R.V X, with mean \mu and variance \sigma^2
If variance is small, X is unlikely to be far from the mean.

P(|X - \mu| \ge c) \le \sigma^2/c^2

Using this inequality, P(|X - \mu| \ge k.\sigma) \le \sigma^2/k^{2}.\sigma^2 = 1/k^2 Thus,
we can also think about this as, proportion of entries greater than k
s.ds away from the mean is bounded by 1/k^2.

In general, this is a better bound than the markov inequality. This is becuase it uses more
information about the r.v.
**** Proof

P(|X - \mu| \ge c) = P((X - \mu)^2 \ge c^2) 

Appying markov inequality,

P((X - \mu)^2 \ge c^2) = E[ (X - \mu)^2 ] / c^2 = \sigma^2/c^2
** The Weak Law of Large numbers (WLLN)
Plays a central role in probability theory.

Start with a distribution with mean \mu and variance \sigma^2

Let X_1, X_2, ... be independent and identically distributed (i.i.d) r.vs.

Sample Mean: M_n = X_1 + X_2 + ... X_n / n
M_n is a function. (since its a function of r.vs)

True mean \mu = E[ X_i ]. i.e the expected value over all possible values of one of the r.vs. (say X_i)
\mu is a number, it is not random.

E[ M_n ] = (E[X1] + E[X2] + ... E[Xn])/ n = n.\mu/n = \mu

Thus, expected value of sample mean is the true mean.

var(M_n) = Var((X1 + ... Xn)/n) = var((X1 + ... Xn))/n^2 = n.\sigma^2/n = \sigma^2/n

From the chebyshev inequality, 

P(|M_n - \mu| > \epsilon) = var(M_n)/\epsilon^2 = \sigma^2/n.\epsilon^2. As n --> \infin, this value tends to 0.

WLLN: For \epsilon > 0, P(|M_n - \mu| > \epsilon) = P(|((X1 + X2 + ... Xn)/ n) - \mu| \ge \epsilon) -> 0, as n -> \infin
*** Interpreting WWLN
One experiment,
- Many measurements X_i = \mu + W_i
- W_i, measurement noise; E[ W_i ] = 0; indepent W_i
- Sample Mean M_n is unlikely to be far off from the true mean \mu.

Many independent repetitions of the same experiment
- event A, with p = P(A)
- X_i: indicator of event A
- True mean E[X_i ] = \mu
- M_n is the empirical frequency of A. 
WWLN tells us that this frequency is unlikely to be far from p.
** Convergence in probability
A sequence Y_n converges in probability to a value /a/ if:

for \epsilon > 0, lim_(n -> \infin) P(|Y_n - a| \ge \epsilon) = 0
*** Some properties
If X_n -> a, Y_n -> b, in probability

If g is continuous, then g(X_n) -> g(a). ex) X_n^2 - >a^2

X_n + Y_n -> a + b

*But* E[ X_n ] need not converge to a.
This makes intuitive sense, as Expected value is affected by a long tail. (extreme values).
But convergence is to do with where the bulk of the probability lies.
** Related topics
Better bounds than Markov and chebyshev inequalities
- Chernoff bound 
- Central limit theorem

Different types of convergence
- convergence in probability
- convergence "with probability 1" (stronger notion of convergence)

Ex) Suppose we have r.vs Y_n, Y. We say we have 'convergence with probability 1' if:
P({w : Y_n(w) -> Y(w) as n -> \infin}) = 1.
- convergence of sequence of CDFs to a limiting CDF
** Central Limit theorem
*** Different views of sum of i.i.d r.vs
X_1, X_2, ... X_n i.i.d mean \mu variance \sigma^2

S_n = X_1 + X_2 + ... X_n
variance n.\sigma^2

M_n = S_n / n
variance = \sigma^2/n

From WWLN, variance -> 0 as n -> \infin

Can we instead try to obtain a more interesting limiting distribution?
lets divide by sqrt(n) instead of n

S_n/sqrt(n) = (X1 + X2 ... Xn) / sqrt(n)
variance: n.\sigma^2/n = \sigma^2 = constant!!
* Bernoulli and Poisson Process
* Markov Chains
** Markov Process
*** Whats so cool about them?
As opposed to bernoulli and poisson process, a markov process allows
some dependency between past and future. This dependency can be
represented by a notion of a 'state', which changes based on some
probability distribution.
*** Discrete time finite state markov chain
=== Markov Chain *From Wikipedia* 
It is a random process that undergoes transitions from one state to
another on a state space. It must possess a property that is usually
characterized as "memorylessness": the probability distribution of the
next state depends only on the current state and not on the sequence
of events that preceded it. This specific kind of "memorylessness" is
called the Markov property. Markov chains have many applications as
statistical models of real-world processes.
===

- We'll stick to the discrete case to keep it simple.

X_n: State of the system after 'n' transitions. (or state at time 'n')
- The state belongs to a finite set. (Set of all possible states (this
  can be infinite, but lets not talk about that)).
- Initial state X_0 either given or random.
- Transition probabilities

p_ij = P(X_1 = j|X_0 = i) = P(X_(n+1)=j|X_n = i) The chain is time
homogeneous. (transition probability is same irrespective of the time)

\Sigma_j p_(ij) = 1

- Markov Property/Assumption
"given current state, the past doesn't matter"

p_(ij) = P(X_(n+)=j|X_n=i) = P(X_(n+)=j|X_n=i, X_(n-1)...X_0)

The markov property to hold in any modelling application, its
important to choose the state carefully.

- Model specification: Identify states, transitions, and transition probabilities.
*** n-step transition probabilities
Given by r_(ij)(n) probability of getting to state 'j' from state 'i' at time 'n'.

r_(ij)(0) = 1 if i = j, 0 otherwise.

r_(ij)(1) = p_(ij) \forall i, \forall j.

r_(ij)(n) = P(X_n = j | X_0 = i) = P(X_(n + s) = j | X_s = i) (because we have a time-invariant markov chain)


Recursion step:

r_(ij)(n) = \Sigma_(k=1 to m)r_(ik)(n-1).p_(kj) (for all k's that have an arc to j)

Alternatively:

r_(ij)(n) = \Sigma_(k=1 to m)p_(ik).r_(kj)(n-1)
*** Random Initial State
The probability of reaching a state j from a random initial state.
P(X_n = j) = \Sigma_(i = 1 to m)P(X_0 = i).r_(ij)(n)
** General Convergence
- For any r_ij(n), as n -> \infin does the probability converge to a constant \pi_j?
This is true in some cases and not true in some cases.
** Transient and Recurrent States
- State i is recurrent if "starting from i, and from wherever you can go, there is a way 
of returning to i".
- If not recurrent, the state is called transient. State i is transient if "starting from i, there
is atleast one way to go away from i and never return to i.
- The recurrent states in a markov chain can be grouped into classes.
Within one class, all recurrent states have a way to communicate with each other.
But recurrent states from multiple classes cannot communicate with one another.
** Steady state behaviour
- In cases where we have multiple recurrent classes,
the steady state behaviour will depend on where we started.
- In cases where we have only one recurrent class, that'll 
depend on certain other properties. (periodicity)
** Periodic states in a recurrent class
The states in a recurrent class are periodic if they can be grouped
into d > 1 groups such that all transitions from one group lead to the
next group.
** Steady State probabilities
Two questions to be asked,
1) Does it converge?
2) Is it independent of the initial state?
Does r_ij(n) = P(X_n = j|X_0=i) converge to some \pi_j?

If we have have 1 recurrent class, and the recurrent class is
aperiodic, then yes. It does converge.

Theorem:
- Recurrent states are all in a single class, and
- single recurrent class is not periodic.

Then the markov chain does converge to a steady-state probability.

The proof is a little complicated, but here is the intuitive idea:

Think about 2 cases with different starting states. After some
transitions, they reach some common state s_i. Once they reach this
state, due to the markov property, they cannot be distinguished from
one another. Probabilistically speaking, they are identical.

Assuming this theorem to be true,
we have:

as n -> \infin, r_ij(n) = \Sigma_(k = 1 to m)r_ik(n - 1).p_kj.

Since this converges, r_ij(n) = \pi_j as n -> \infin
Also, as n -> \infin, r_ij(n-1) = \pi_k

Thus, \pi_j = \Sigma_k \pi_k.p_kj for j = 1, ... m

m equations, m unknowns. (this is singular, so has multiple solutions)
We impose the additional condition that all of these are probabilities. (this forces a unique solution)

\Sigma_(k=1 to m)\pi_j = 1

